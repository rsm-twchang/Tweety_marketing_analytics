---
title: "Poisson Regression Examples"
author: "Daniel Garcia"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
editor_options: 
  chunk_output_type: console
---

## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. unfortunately, such data is not available.

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.

### Data

```{r include=FALSE}
# Load necessary libraries
library(dplyr)     
library(ggplot2)  
library(foreign) 
library(MASS)
library(tidyverse)
library(knitr)
library(kableExtra)
library(gridExtra)
library(broom)
library(stats)
library(readr)

# Load the data
data <- read.csv("/Users/danielgarciagomar/Documents/UCSD/MPP/Spring/Marketing Analytics/Quarto/quarto_website/projects/project2/blueprinty.csv")

# Assuming the relevant columns are named 'patents', 'uses_blueprinty' ('yes' or 'no')
# Check structure of the data
str(data)

# Calculate mean number of patents for each group
mean_patents <- data %>%
  group_by(iscustomer) %>%
  summarise(MeanPatents = mean(patents, na.rm = TRUE))

# Print the results
print(mean_patents)

```

Based on the previous histogram, we can observe that the patents are skewed to the left which does not show a normal distribution. Based on the analysis of the histograms and mean number of patents awarded to firms, we observe that firms using Blueprinty's software, on average, have a higher number of patents (4 patents) compared to those not using the software (3.6 patents). The histograms likely show that the distribution for firms using Blueprinty is shifted slightly to the right, indicating a higher frequency of firms with a greater number of patents. This analysis, bolstered by the statistically significant results of the t-test, strongly suggests a positive association between the use of Blueprinty’s software and the number of patents awarded. The evidence supports the claim that using Blueprinty’s software may enhance a firm’s success in securing patents.

```{r echo=FALSE, warning=FALSE}
# Histograms to compare the number of patents
ggplot(data, aes(x = patents, fill = iscustomer)) +
  geom_histogram(position = "dodge", binwidth = 1) + # Adjust binwidth based on data
  labs(title = "Distribution of Patents by Usage of Blueprinty Software",
       x = "Number of Patents",
       y = "Frequency",
       fill = "Uses Blueprinty")
```


```{r echo=FALSE}
kable(mean_patents, caption = "Mean Number of Patents by Usage of Blueprinty Software", 
      format = "html", # Use "latex" for LaTeX output, or "markdown" for Markdown
      col.names = c("Uses Blueprinty", "Mean Number of Patents"))
```

```{r include=FALSE}
# Assuming 'uses_blueprinty' is a factor indicating if the firm uses Blueprinty ('yes' or 'no')
data$iscustomer <- as.factor(data$iscustomer)

# Conduct an independent two-sample t-test
t_test_results <- t.test(patents ~ iscustomer, data = data,
                         alternative = "two.sided",  # Tests for a difference in means in either direction
                         conf.level = 0.95)          # 95% confidence interval

# Print the results
print(t_test_results)

# Tidy the t-test results
tidy_t_test_results <- tidy(t_test_results)

```

Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers. After examining the data on age and regional distribution related to the status of Blueprinty software users, distinct patterns emerge that shed light on the user demographics and geographical preferences. Firms that use Blueprinty's software have an average age of 24.15 years, which is notably younger than the 26.69 years for those that do not use the software. This indicates a trend where younger firms are more likely to embrace Blueprinty, potentially due to their greater openness to adopting innovative technologies or their different needs in managing patent applications effectively.

The regional analysis reveals a consistent trend across various U.S. regions: non-users of Blueprinty's software significantly outnumber users. For example, in the Midwest, there are 207 non-users to just 17 users; in the Northeast, 488 non-users to 113 users; and this pattern holds in the Northwest, South, and Southwest as well. These figures suggest that while Blueprinty has a presence across these regions, its market penetration varies and remains relatively low compared to the potential number of firms that could benefit from its services.

The combination of these age and regional data points to a clear profile of Blueprinty's current market which is younger firms across various regions, though still a minority among potential users. This demographic and regional information could be crucial for Blueprinty in tailoring its marketing strategies and product offerings. Specifically, it suggests a strategic opportunity to focus on younger firms and increase efforts in regions with lower adoption rates, possibly by addressing regional specific needs or barriers to adoption. Overall, the data provides a roadmap for potential growth and deeper market penetration for Blueprinty's innovative software solutions.


```{r include=FALSE}
# Summary statistics for age
age_summary <- data %>%
  group_by(iscustomer) %>%
  summarise(MeanAge = mean(age, na.rm = TRUE), MedianAge = median(age, na.rm = TRUE), 
            AgeSD = sd(age, na.rm = TRUE))

# Print age summary
print(age_summary)

# Regional distribution by customer status
region_summary <- data %>%
  group_by(region, iscustomer) %>%
  summarise(Count = n()) %>%
  ungroup()

# Print region summary
print(region_summary)

```

```{r include=FALSE}
# Render the age summary table
kable(age_summary, 
      caption = "Summary Statistics for Age by Customer Status",
      format = "html",  # Change to "latex" for LaTeX output or "markdown" for Markdown
      col.names = c("Customer Status", "Mean Age", "Median Age", "Standard Deviation"),
      digits = 2)  # Adjust the number of decimal places as needed

# Render the regional distribution summary table
kable(region_summary, 
      caption = "Regional Distribution by Customer Status",
      format = "html",  # Change to "latex" for LaTeX output or "markdown" for Markdown
      col.names = c("Region", "Customer Status", "Count"),
      digits = 0)  # Counts typically are integers, so no decimal places are needed
```

```{r echo=FALSE}
# Plotting
# Age distribution by customer status
ggplot(data, aes(x = iscustomer, y = age)) +
  geom_boxplot(fill = c("skyblue", "orange")) +
  labs(title = "Age Distribution by Customer Status", x = "Uses Blueprinty Software", y = "Age")

# Regional distribution by customer status
ggplot(region_summary, aes(x = region, y = Count, fill = iscustomer)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Regional Distribution by Customer Status", x = "Region", y = "Count")
```


### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

$Y \sim \text{Poisson}(\lambda)$. Note that $f(Y|\lambda) = e^{-\lambda}\lambda^Y/Y!$.

The log-likelihood function for a set of independent observations $y_1, y_2, \ldots, y_n$ from a Poisson distribution with parameter $\lambda$ is given by:

$$
\ell(\lambda | y_1, y_2, \ldots, y_n) = \log L(\lambda | y_1, y_2, \ldots, y_n) = -n\lambda + \left(\sum_{i=1}^n y_i\right) \log \lambda - \sum_{i=1}^n \log(y_i!)
$$


```{r}
#Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example
poisson_loglikelihood <- function(lambda, Y) {
    if(lambda <= 0) {
        return(-Inf)  # log-likelihood is -infinity if lambda is not positive
    }
    
    # Calculate the log-likelihood for Poisson distribution
    log_likelihood <- -length(Y) * lambda + sum(Y * log(lambda)) - sum(lfactorial(Y))
    return(log_likelihood)
}

```


```{r echo=FALSE}
# Extract the 'patents' column
Y <- data$patents

# Generate a sequence of lambda values for which to compute the log-likelihood
lambda_values <- seq(0.1, 20, by = 0.1)

# Calculate log-likelihoods for each lambda in lambda_values
log_likelihoods <- sapply(lambda_values, function(l) poisson_loglikelihood(l, Y))

# Create a data frame for plotting
likelihood_data <- data.frame(Lambda = lambda_values, LogLikelihood = log_likelihoods)

# Plot the log-likelihood as a function of lambda
ggplot(likelihood_data, aes(x = Lambda, y = LogLikelihood)) +
    geom_line() +
    labs(title = "Log-Likelihood of Poisson Model for Various Lambda",
         x = "Lambda (λ)",
         y = "Log-Likelihood")
```


Given the log-likelihood function for a set of observations from a Poisson distribution:
$$
\ell(\lambda) = -n\lambda + \left(\sum_{i=1}^n y_i\right) \log \lambda - \sum_{i=1}^n \log(y_i!)
$$

The first derivative with respect to \(\lambda\) is:
$$
\frac{\partial \ell}{\partial \lambda} = -n + \frac{\sum_{i=1}^n y_i}{\lambda}
$$

Setting this derivative equal to zero for maximization:
$$
-n + \frac{\sum_{i=1}^n y_i}{\lambda} = 0
$$

Solving for \(\lambda\):
$$
\lambda = \frac{\sum_{i=1}^n y_i}{n}
$$

Thus, the MLE of \(\lambda\) is the sample mean, \(\overline{y}\), which matches the expectation of the Poisson parameter:
$$
\lambda_{\text{MLE}} = \overline{y}
$$

```{r}
# Calculate the MLE for lambda, which is the mean of Y
lambda_mle <- mean(Y)

# Output the result
print(paste("The MLE of lambda is:", lambda_mle))
```


```{r echo=FALSE}
# Extract the 'patents' column from the dataset
Y <- data$patents

# Define the negative log-likelihood function for Poisson distribution
neg_log_likelihood <- function(lambda, Y) {
    if (lambda <= 0) return(Inf)  # Return a large number if lambda is not positive
    return(-(-length(Y) * lambda + sum(Y * log(lambda)) - sum(lfactorial(Y))))
}
# Initial guess for lambda
initial_lambda <- mean(Y)

# Use optim to maximize the log-likelihood
mle_results <- optim(par = initial_lambda, fn = neg_log_likelihood, Y = Y, method = "BFGS")

# The estimated value of lambda (MLE)
lambda_mle <- mle_results$par

# Print the MLE of lambda
print(paste("The MLE of lambda is:", lambda_mle))

# Check convergence
if (mle_results$convergence == 0) {
    print("Optimization successful!")
} else {
    print("Optimization may have failed; check results.")
}
```


### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g() to be exp() so that* $\lambda_i = e^{X_i'\beta}$. For example:*

```{r}
poisson_regression_likelihood <- function(beta, Y, X) {
    lambda <- exp(X %*% beta)
    
    log_likelihood <- sum(Y * log(lambda) - lambda - lfactorial(Y))
    
    # Return the negative of log-likelihood because most R optimization functions minimize
    return(-log_likelihood)
}
```


```{r include=FALSE}
#*todo: Use your function along with R's optim() or Python's sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1's to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.*


library(fastDummies) 

data <- read.csv("/Users/danielgarciagomar/Documents/UCSD/MPP/Spring/Marketing Analytics/Quarto/quarto_website/projects/project2/blueprinty.csv")

# Ensure 'region' is a factor if not already
data$region <- as.factor(data$region)

# Create a model matrix, including the intercept
# We use model.matrix to correctly handle factor variables and include an intercept
X <- model.matrix(~ age + I(age^2) + region + iscustomer, data = data)

# Check the structure of X to ensure it's all numeric
str(X)

# Response vector Y
Y <- data$patents

# Ensure Y is numeric
Y <- as.numeric(Y)

poisson_log_likelihood <- function(beta, Y, X) {
    lambda <- exp(X %*% beta)
    log_likelihood <- sum(Y * log(lambda) - lambda - lfactorial(Y))
    return(-log_likelihood)
}

# Initial guess for beta parameters, should match the number of predictors
initial_beta <- rep(0, ncol(X))

# Optimization using optim()
optim_results <- optim(par = initial_beta, fn = poisson_log_likelihood, Y = Y, X = X, method = "BFGS", hessian = TRUE)

# Invert the Hessian to get the covariance matrix
cov_matrix <- solve(optim_results$hessian)

# Standard errors are the square roots of the diagonal elements of the covariance matrix
standard_errors <- sqrt(diag(cov_matrix))

# Extract the estimates
beta_estimates <- optim_results$par

# Create a data frame to display beta estimates and their standard errors
results_table <- data.frame(Estimate = beta_estimates, StdError = standard_errors)
rownames(results_table) <- colnames(X)  # Ensuring the row names match predictor names

# Display the results
print(results_table)
```

```{r echo=FALSE}

kable(results_table, 
      caption = "Estimated Coefficients and Standard Errors for Poisson Regression Model", 
      format = "html",  # Use "latex" for LaTeX output, or "markdown" for Markdown
      col.names = c("Estimate", "Standard Error"),
      align = 'c')

```


```{r include=FALSE}
#Check your results using R's glm() function or Python sm.GLM() function.*
# Fit the Poisson regression model
poisson_model <- glm(patents ~ age + I(age^2) + region + iscustomer, family = poisson(link = "log"), data = data)

# Summary of the model to check results
summary(poisson_model)
```

```{r echo=FALSE}

# Extract coefficients and standard errors
coef_table <- summary(poisson_model)$coefficients

# Create a table using knitr::kable()
kable(coef_table, caption = "GLM Poisson Regression Results", format = "html", 
      col.names = c("Estimate", "Std. Error", "z value", "Pr(>|z|)"),
      digits = 4)

```


The results from the Poisson regression models provide valuable insights into the factors influencing patent success among engineering firms. Notably, the use of Blueprinty's software is significantly associated with increased patent counts. Specifically, the GLM analysis reveals that being a customer of Blueprinty’s software increases the log count of patents significantly, with a coefficient of 0.1181 (p = 0.0024), indicating a positive and statistically significant impact on patent awards when controlling for other factors. This finding supports the marketing claim that Blueprinty’s software enhances a firm's ability to secure patents.

Additionally, the age of a firm and its squared term have significant effects on patent success, suggesting a quadratic relationship where patent activity increases with firm age up to a point before it begins to decline. This lifecycle effect is crucial for understanding the dynamics of innovation as firms mature. Regional differences also play a role, with firms in the Northeast experiencing higher patent counts compared to the baseline region, suggesting regional disparities in patent activities.

Overall, these results underscore the effectiveness of Blueprinty's software in boosting patent success, while also highlighting the importance of firm age and regional context in patent award dynamics. This analysis not only reinforces the value proposition of Blueprinty's software but also provides strategic insights for targeting specific firm demographics and regions to maximize patent production.

## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:

::: {.callout-note collapse="true"}
### Variable Definitions

```         
- `id` = unique ID number for each unit
- `last_scraped` = date when information scraped
- `host_since` = date when host first listed the unit on Airbnb
- `days` = `last_scraped` - `host_since` = number of days the unit has been listed
- `room_type` = Entire home/apt., Private room, or Shared room
- `bathrooms` = number of bathrooms
- `bedrooms` = number of bedrooms
- `price` = price per night (dollars)
- `number_of_reviews` = number of reviews for the unit on Airbnb
- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
- `review_scores_location` = a "quality of location" score from reviews (1-10)
- `review_scores_value` = a "quality of value" score from reviews (1-10)
- `instant_bookable` = "t" if instantly bookable, "f" if not
```
:::

```{r include=FALSE}
# Load the data
airbnb_data <- read_csv("/Users/danielgarciagomar/Documents/UCSD/MPP/Spring/Marketing Analytics/Quarto/quarto_website/projects/project2/airbnb.csv")

# Check for missing values
colSums(is.na(airbnb_data))

# Drop rows with any missing values in critical columns
airbnb_data <- airbnb_data %>%
  drop_na(bedrooms, bathrooms, price, number_of_reviews, review_scores_cleanliness, review_scores_location, review_scores_value)

# Inspect the data
summary(airbnb_data)
str(airbnb_data)
```

### Descriptive

```{r}
# Visualize distributions of numerical variables
ggplot(airbnb_data, aes(x = price)) + geom_histogram(bins = 30) + ggtitle("Distribution of Prices")
```

```{r}
# Histogram of the Number of Reviews
ggplot(airbnb_data, aes(x = number_of_reviews)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Number of Reviews", x = "Number of Reviews", y = "Count")

# Histogram of Bedrooms
ggplot(airbnb_data, aes(x = bedrooms)) +
  geom_histogram(bins = 10, fill = "coral", color = "black") +
  labs(title = "Distribution of Bedrooms", x = "Number of Bedrooms", y = "Count")

# Histogram of Bathrooms
ggplot(airbnb_data, aes(x = bathrooms)) +
  geom_histogram(bins = 10, fill = "lightgreen", color = "black") +
  labs(title = "Distribution of Bathrooms", x = "Number of Bathrooms", y = "Count")
```

```{r}
ggplot(airbnb_data, aes(x = price, y = number_of_reviews)) +
  geom_point(alpha = 0.5, color = "blue") +
  labs(title = "Price vs. Number of Reviews", x = "Price ($)", y = "Number of Reviews")
```


```{r}
# Explore relationships between number of reviews and other features
ggplot(airbnb_data, aes(x = bedrooms, y = number_of_reviews)) + geom_point() + ggtitle("Bedrooms vs. Number of Reviews")
```

```{r}
ggplot(airbnb_data, aes(x = instant_bookable, fill = instant_bookable)) +
  geom_bar() +
  labs(title = "Effect of Instant Bookable Feature on Listing Counts", x = "Instant Bookable", y = "Count of Listings")
```

### Analysis

```{r}
# Fit the model
poisson_model <- glm(number_of_reviews ~ bedrooms + bathrooms + price + review_scores_cleanliness + review_scores_location + review_scores_value + instant_bookable, family = poisson(link = "log"), data = airbnb_data)

# Check summary
summary(poisson_model)
```

TThe fitted Poisson regression model offers significant insights into the dynamics affecting the number of reviews on Airbnb listings, with the intercept showing a substantial baseline log count of reviews of 3.543 when all predictors are held constant. This high intercept suggests that even in the absence of any enhancing factors, listings inherently attract a baseline level of reviews.

Delving into the effects of specific variables, it is clear that larger properties, as indicated by the number of bedrooms, tend to attract more reviews, with each additional bedroom leading to a roughly 7.82% increase in the number of reviews. This trend underscores the preference or need among Airbnb users for more spacious accommodations, which likely accommodate larger groups who may be more engaged in leaving feedback. In contrast, an increase in the number of bathrooms tends to reduce the number of reviews by about 12.86%. This might suggest that properties with disproportionately more bathrooms than bedrooms are perceived as offering less value, particularly to larger groups who are key contributors to reviews. The impact of pricing on reviews is minimal and statistically insignificant, indicating that the nightly rate, within the range observed in the dataset, does little to influence the frequency of reviews. This might point to price sensitivity being less critical than other factors in determining guest satisfaction or engagement.

Review scores significantly affect the number of reviews, with cleanliness being particularly critical. Each additional point in cleanliness scores correlates with an 11.35% increase in reviews, highlighting the paramount importance guests place on cleanliness. However, higher location and value scores are associated with fewer reviews. This could reflect a scenario where high expectations set by top ratings in these categories are not always met, or perhaps satisfaction in these areas leads to less perceived need among guests to provide feedback.

The feature of instant bookability markedly enhances the number of reviews, with listings offering this option seeing a 33.19% increase in reviews compared to those without it. The convenience of instant booking appears to be a significant draw for guests, leading to higher engagement in terms of reviews.While the model's Akaike Information Criterion (AIC) and residual deviance indicate a good fit, there is room for improvement. This could potentially be achieved by incorporating interaction terms or exploring other unobserved factors that might refine the understanding of what drives reviews on Airbnb.

Overall, the model elucidates several key insights: practical features like more bedrooms and instant bookability tend to increase reviews, while more subjective assessments such as location and value can detract from them if guest expectations are not managed appropriately. The critical role of cleanliness in driving review counts cannot be overstated. This comprehensive analysis can assist hosts in focusing on the attributes most valued by guests, potentially guiding enhancements that lead to more bookings and feedback, thereby boosting the visibility and success of their listings

```{r}
nb_model <- glm.nb(number_of_reviews ~ bedrooms + bathrooms + price + review_scores_cleanliness + review_scores_location + review_scores_value + instant_bookable, data = airbnb_data)

# Check summary
summary(nb_model)
```

The analysis conducted using the Negative Binomial regression model reveals significant insights about the factors that influence the number of reviews on Airbnb listings. The model, robust in its statistical significance and precision, illuminates the impacts of various property characteristics on guest engagement. The intercept of the model is notably high and statistically significant, indicating a substantial baseline number of reviews independent of the property features. This high baseline suggests that listings on Airbnb tend to attract a certain level of reviews due to the platform's inherent popularity and the active participation of its user base.

Among the variables analyzed, the number of bedrooms in a property shows a positive correlation with the number of reviews. Specifically, each additional bedroom is associated with a significant increase in the expected number of reviews, highlighting that larger properties, which typically accommodate more guests, are more likely to receive feedback, possibly due to the increased likelihood of group travels or family stays. An increase in the number of bathrooms tends to decrease the number of reviews. This negative association might suggest that properties with a higher number of bathrooms relative to their bedrooms do not provide proportional value or appeal, especially if these additional bathrooms lead to higher rental prices without corresponding benefits. The pricing variable shows a very small and statistically insignificant effect on the number of reviews, indicating that price fluctuations within the observed range do not significantly impact guest decisions to leave reviews. This suggests that guests' decisions to review are driven more by their experiences than by cost considerations.

Review scores also play a critical role in influencing reviews. High cleanliness scores are particularly impactful, with each point increase leading to a substantial rise in the number of reviews. This underscores the importance of cleanliness in guest satisfaction and their propensity to leave feedback. In contrast, higher scores for location and value are associated with fewer reviews. This could be due to the high expectations set by such scores not being met or that guests satisfied with the location and value may feel less compelled to leave feedback. The feature of instant bookability significantly boosts the number of reviews, with listings offering this option seeing a marked increase in reviews compared to those that do not. The convenience and ease of booking facilitated by this feature likely encourage more bookings and consequently, more reviews.

Overall, the model demonstrates a good fit, as evidenced by the AIC and the closeness of the null and residual deviances, suggesting it captures the variation in review numbers well. The significant theta value, indicating the appropriateness of the Negative Binomial model over simpler models, points to the presence of overdispersion in the count data, which this model adequately addresses.

In conclusion, the findings reveal that while practical amenities like additional bedrooms and instant bookability enhance review counts, subjective assessments such as location and value can detract from them if not managed properly. Cleanliness emerges as a key driver of reviews, highlighting an area where hosts can focus their efforts to boost guest satisfaction and attract more reviews. This comprehensive analysis provides actionable insights for Airbnb hosts aiming to optimize their listings and improve guest experiences.