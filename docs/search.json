[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tweety Chang",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "hw2_questions.html",
    "href": "hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Read the data \ndf = pd.read_csv('blueprinty.csv')\n\n# Check structure of the data\ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n# Calculate mean number of patents for each group (assuming 'iscustomer' is the same as 'uses_blueprinty')\nmean_patents = df.groupby('iscustomer')['patents'].mean().reset_index(name='MeanPatents')\n# Print the results\nprint(mean_patents)\n\n   iscustomer  MeanPatents\n0           0     3.473013\n1           1     4.133056\n\n\n\n\n\n\n\n\n\n\n\nThe histogram of patent counts shows a right-skewed distribution, which is expected for count data like the number of patents. Firms using Blueprinty’s software tend to have a distribution that is slightly shifted to the right, indicating they may be more likely to have higher patent counts.\nWhen comparing group means, firms that use Blueprinty’s software have an average of approximately 4 patents, compared to 3.6 patents for non-users. While this difference is modest, it does suggest a potential positive association between using the software and patenting success.\nHowever, this visual and descriptive comparison alone is not enough to establish a causal relationship. Further statistical analysis — such as a Poisson regression or a formal hypothesis test — is needed to determine whether the observed difference is statistically and practically significant, controlling for other variables like firm age and region.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\nAge Summary by Customer Status:\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\niscustomer\n\n\n\n\n\n\n\n\n\n\n\n\n0\n1019.0\n26.101570\n6.945426\n9.0\n21.0\n25.5\n31.25\n47.5\n\n\n1\n481.0\n26.900208\n7.814678\n10.0\n20.5\n26.5\n32.50\n49.0\n\n\n\n\n\n\n\n\n\n\nRegion Distribution by Customer Status:\n\n\n\n\n\n\n\n\niscustomer\n0\n1\nAll\n\n\nregion\n\n\n\n\n\n\n\nMidwest\n187\n37\n224\n\n\nNortheast\n273\n328\n601\n\n\nNorthwest\n158\n29\n187\n\n\nSouth\n156\n35\n191\n\n\nSouthwest\n245\n52\n297\n\n\nAll\n1019\n481\n1500\n\n\n\n\n\n\n\n\n\n\n\nWe are interested in modeling the number of patents awarded to each engineering firm over a fixed 5-year period. Since this outcome is a non-negative count variable, the Poisson distribution is a natural choice. It is well-suited for modeling events that occur independently and randomly over time or space.\nWe begin by estimating a simple Poisson model using Maximum Likelihood Estimation (MLE). Let: Y () where is the expected number of patents for a firm.\nThe probability mass function of the Poisson distribution is: f(Y ) = \nGiven n independent observations y_1, y_2, , y_n, the log-likelihood function is:\n\\[\n\\ell(\\lambda \\mid y_1, \\ldots, y_n) = -n\\lambda + \\left( \\sum_{i=1}^n y_i \\right) \\log \\lambda - \\sum_{i=1}^n \\log(y_i!)\n\\]\nThis expression captures the likelihood of observing the data as a function of , which we estimate by maximizing ().\n\nfrom scipy.special import gammaln  \n\ndef poisson_loglikelihood(lmbda, Y):\n    \"\"\"\n    Computes the log-likelihood of a Poisson model.\n    \n    Parameters:\n    - lmbda: scalar or array-like (same length as Y), expected rate parameter(s)\n    - Y: array-like, observed count data\n\n    Returns:\n    - log-likelihood value (scalar)\n    \"\"\"\n    lmbda = np.asarray(lmbda)\n    Y = np.asarray(Y)\n    \n    # Ensure shape compatibility\n    if np.isscalar(lmbda):\n        lmbda = np.full_like(Y, lmbda, dtype=np.float64)\n    \n    # Compute log-likelihood\n    loglik = np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n    return loglik\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('blueprinty.csv')\nY = df['patents'].values  # observed count data\n# Evaluate log-likelihood over a range of lambda values\nlambda_values = np.linspace(0.1, 10, 200)  # avoid zero to prevent log(0)\nloglik_values = [poisson_loglikelihood(lmb, Y) for lmb in lambda_values]\n\n# Plotting\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_values, loglik_values, label='Log-Likelihood')\nplt.axvline(np.mean(Y), color='red', linestyle='--', label='Sample Mean (MLE)')\nplt.title('Poisson Log-Likelihood vs. Lambda')\nplt.xlabel('Lambda')\nplt.ylabel('Log-Likelihood')\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nTo estimate the Poisson rate parameter , we use Maximum Likelihood Estimation (MLE). The log-likelihood function for a sample y_1, y_2, , y_n drawn independently from a Poisson distribution is:\n\\[\n\\ell(\\lambda) = -n\\lambda + \\left(\\sum_{i=1}^n y_i\\right) \\log \\lambda - \\sum_{i=1}^n \\log(y_i!)\n\\]\nTo find the value of that maximizes this likelihood, we take the first derivative with respect to :\n\\[\n\\frac{\\partial \\ell}{\\partial \\lambda} = -n + \\frac{\\sum_{i=1}^n y_i}{\\lambda}\n\\]\nSetting the derivative equal to zero gives the critical point:\n\\[\n-n + \\frac{\\sum_{i=1}^n y_i}{\\lambda} = 0\n\\]\nSolving for , we obtain:\n\\[\n\\lambda = \\frac{\\sum_{i=1}^n y_i}{n}\n\\]\nThus, the maximum likelihood estimator (MLE) for is the sample mean, :\n\\[\n\\lambda_{\\text{MLE}} = \\overline{y}\n\\]\nThis result aligns with our intuition: in a Poisson distribution, the mean and variance are both equal to , so the best estimate for the average rate of occurrence is the observed average in the data.\n\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\n\n# Define the negative log-likelihood function\ndef neg_log_likelihood(lmbda, Y):\n    lmbda = lmbda[0]  # Extract scalar from array\n    if lmbda &lt;= 0:\n        return np.inf\n    return -np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n# Initial guess (sample mean)\ninitial_lambda = np.array([np.mean(Y)])\n# Perform the optimization\nresult = minimize(fun=neg_log_likelihood, x0=initial_lambda, args=(Y,), method='BFGS')\n# Extract results\nlambda_mle = result.x[0]\n# Print the MLE\nprint(f\"The MLE of lambda is: {lambda_mle:.4f}\")\n\nThe MLE of lambda is: 3.6847\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\ndef poisson_regression_neg_loglikelihood(beta, Y, X):\n    \"\"\"\n    Computes the negative log-likelihood for Poisson regression.\n    \n    Parameters:\n    - beta: array-like, shape (p,), model coefficients\n    - Y: array-like, shape (n,), observed counts\n    - X: array-like, shape (n, p), design matrix of covariates\n    \n    Returns:\n    - Negative log-likelihood (scalar)\n    \"\"\"\n    \n    beta = np.asarray(beta)\n    X = np.asarray(X)\n    Y = np.asarray(Y)    \n    # Compute lambda_i = exp(X_i^T * beta)\n    lambda_ = np.exp(X @ beta)    \n    # Compute log-likelihood\n    log_likelihood = np.sum(Y * np.log(lambda_) - lambda_ - gammaln(Y + 1))   \n    return -log_likelihood  \n\n\n\n         Current function value: nan\n         Iterations: 1\n         Function evaluations: 1008\n         Gradient evaluations: 112\n\nPoisson Regression Coefficients and Standard Errors:\n\n\n\n\n\n\n\n\n\nEstimate\nStdError\n\n\n\n\nIntercept\n1.4801\n1.0\n\n\nC(region)[T.Northeast]\n0.6410\n1.0\n\n\nC(region)[T.Northwest]\n0.1643\n1.0\n\n\nC(region)[T.South]\n0.1816\n1.0\n\n\nC(region)[T.Southwest]\n0.2955\n1.0\n\n\nage\n38.0164\n1.0\n\n\nI(age ** 2)\n1033.5396\n1.0\n\n\niscustomer\n0.5539\n1.0\n\n\n\n\n\n\n\n\nfrom IPython.display import display\n\n# Rename columns to match your desired output\nresults_df.columns = [\"Estimate\", \"Standard Error\"]\n\n# Display nicely with caption\ndisplay(results_df.style.set_caption(\"Estimated Coefficients and Standard Errors for Poisson Regression Model\")\n                        .set_properties(**{'text-align': 'center'}))\n\n\n\n\n\n\nTable 1: Estimated Coefficients and Standard Errors for Poisson Regression Model\n\n\n\n\n\n \nEstimate\nStandard Error\n\n\n\n\nIntercept\n1.480059\n1.000000\n\n\nC(region)[T.Northeast]\n0.640979\n1.000000\n\n\nC(region)[T.Northwest]\n0.164288\n1.000000\n\n\nC(region)[T.South]\n0.181562\n1.000000\n\n\nC(region)[T.Southwest]\n0.295497\n1.000000\n\n\nage\n38.016417\n1.000000\n\n\nI(age ** 2)\n1033.539585\n1.000000\n\n\niscustomer\n0.553874\n1.000000\n\n\n\n\n\n\n\n\nThe estimated coefficient for iscustomer is 0.5539, suggesting that firms using Blueprinty’s software have a higher expected number of patents. Since Poisson regression models the log of expected counts, we exponentiate the coefficient to interpret it on the original scale: e^{0.5539} \nThis implies that, holding age and region constant, Blueprinty customers are expected to receive 74% more patents than non-customers. However, the magnitude of the coefficients for age (38.02) and age² (1033.54) is unusually large and likely reflects scaling issues. This suggests the model may benefit from centering or standardizing the age variable. Finally, the fact that all standard errors are reported as 1.000 raises a concern: it’s likely that results_df was not constructed from actual model output and should be validated to ensure correct inference.\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                patents   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Mon, 05 May 2025   Deviance:                       2143.3\nTime:                        23:24:28   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nconst               -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage                  0.1486      0.014     10.716      0.000       0.121       0.176\nage_squared         -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer           0.2076      0.031      6.719      0.000       0.147       0.268\nregion_Northeast     0.0292      0.044      0.669      0.504      -0.056       0.115\nregion_Northwest    -0.0176      0.054     -0.327      0.744      -0.123       0.088\nregion_South         0.0566      0.053      1.074      0.283      -0.047       0.160\nregion_Southwest     0.0506      0.047      1.072      0.284      -0.042       0.143\n====================================================================================\n\n\nThe Poisson regression results provide strong evidence that using Blueprinty’s software is associated with increased patenting activity among engineering firms. Firms that use the software are expected to produce approximately 23% more patents over a five-year period, even after accounting for firm age and regional location. This finding supports the claim that Blueprinty’s software contributes positively to patent success.\n\n\n\n\n\n\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n[0.025\n0.975]\n\n\n\n\nconst\n-0.5089\n0.1832\n-2.7783\n0.0055\n-0.8679\n-0.1499\n\n\nage\n0.1486\n0.0139\n10.7162\n0.0000\n0.1214\n0.1758\n\n\nage_squared\n-0.0030\n0.0003\n-11.5132\n0.0000\n-0.0035\n-0.0025\n\n\niscustomer\n0.2076\n0.0309\n6.7192\n0.0000\n0.1470\n0.2681\n\n\nregion_Northeast\n0.0292\n0.0436\n0.6686\n0.5037\n-0.0563\n0.1147\n\n\nregion_Northwest\n-0.0176\n0.0538\n-0.3268\n0.7438\n-0.1230\n0.0878\n\n\nregion_South\n0.0566\n0.0527\n1.0740\n0.2828\n-0.0467\n0.1598\n\n\nregion_Southwest\n0.0506\n0.0472\n1.0716\n0.2839\n-0.0419\n0.1431\n\n\n\n\n\nThe Poisson regression analysis provides clear evidence that the use of Blueprinty’s software is significantly associated with higher patent output among engineering firms. The coefficient for iscustomer (0.2076, p &lt; 0.001) suggests that firms using Blueprinty’s tools are expected to produce approximately 23% more patents, holding other factors constant. This finding offers strong empirical support for the claim that Blueprinty’s software contributes to improved patenting outcomes.\nFirm age is also an important predictor. The positive coefficient for age and the negative coefficient for age squared indicate a nonlinear relationship: patent activity increases as firms mature but eventually slows down—reflecting a typical lifecycle pattern of innovation intensity.\nIn contrast, the analysis finds no statistically significant differences across regions, suggesting that regional location does not meaningfully influence patent success once firm-level factors are considered.\n\n\n\nOverall, these findings reinforce the value of Blueprinty’s software as a meaningful contributor to patent productivity. At the same time, they highlight the role of firm maturity in shaping innovation outcomes. While regional variation appears limited, the analysis underscores the importance of targeting firms at the right stage of development and aligning product value with their innovation capacity. These insights can guide Blueprinty’s strategic messaging and outreach efforts, particularly when engaging established firms seeking to strengthen their patent portfolios."
  },
  {
    "objectID": "hw2_questions.html#blueprinty-case-study",
    "href": "hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Read the data \ndf = pd.read_csv('blueprinty.csv')\n\n# Check structure of the data\ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n# Calculate mean number of patents for each group (assuming 'iscustomer' is the same as 'uses_blueprinty')\nmean_patents = df.groupby('iscustomer')['patents'].mean().reset_index(name='MeanPatents')\n# Print the results\nprint(mean_patents)\n\n   iscustomer  MeanPatents\n0           0     3.473013\n1           1     4.133056\n\n\n\n\n\n\n\n\n\n\n\nThe histogram of patent counts shows a right-skewed distribution, which is expected for count data like the number of patents. Firms using Blueprinty’s software tend to have a distribution that is slightly shifted to the right, indicating they may be more likely to have higher patent counts.\nWhen comparing group means, firms that use Blueprinty’s software have an average of approximately 4 patents, compared to 3.6 patents for non-users. While this difference is modest, it does suggest a potential positive association between using the software and patenting success.\nHowever, this visual and descriptive comparison alone is not enough to establish a causal relationship. Further statistical analysis — such as a Poisson regression or a formal hypothesis test — is needed to determine whether the observed difference is statistically and practically significant, controlling for other variables like firm age and region.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\nAge Summary by Customer Status:\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\niscustomer\n\n\n\n\n\n\n\n\n\n\n\n\n0\n1019.0\n26.101570\n6.945426\n9.0\n21.0\n25.5\n31.25\n47.5\n\n\n1\n481.0\n26.900208\n7.814678\n10.0\n20.5\n26.5\n32.50\n49.0\n\n\n\n\n\n\n\n\n\n\nRegion Distribution by Customer Status:\n\n\n\n\n\n\n\n\niscustomer\n0\n1\nAll\n\n\nregion\n\n\n\n\n\n\n\nMidwest\n187\n37\n224\n\n\nNortheast\n273\n328\n601\n\n\nNorthwest\n158\n29\n187\n\n\nSouth\n156\n35\n191\n\n\nSouthwest\n245\n52\n297\n\n\nAll\n1019\n481\n1500\n\n\n\n\n\n\n\n\n\n\n\nWe are interested in modeling the number of patents awarded to each engineering firm over a fixed 5-year period. Since this outcome is a non-negative count variable, the Poisson distribution is a natural choice. It is well-suited for modeling events that occur independently and randomly over time or space.\nWe begin by estimating a simple Poisson model using Maximum Likelihood Estimation (MLE). Let: Y () where is the expected number of patents for a firm.\nThe probability mass function of the Poisson distribution is: f(Y ) = \nGiven n independent observations y_1, y_2, , y_n, the log-likelihood function is:\n\\[\n\\ell(\\lambda \\mid y_1, \\ldots, y_n) = -n\\lambda + \\left( \\sum_{i=1}^n y_i \\right) \\log \\lambda - \\sum_{i=1}^n \\log(y_i!)\n\\]\nThis expression captures the likelihood of observing the data as a function of , which we estimate by maximizing ().\n\nfrom scipy.special import gammaln  \n\ndef poisson_loglikelihood(lmbda, Y):\n    \"\"\"\n    Computes the log-likelihood of a Poisson model.\n    \n    Parameters:\n    - lmbda: scalar or array-like (same length as Y), expected rate parameter(s)\n    - Y: array-like, observed count data\n\n    Returns:\n    - log-likelihood value (scalar)\n    \"\"\"\n    lmbda = np.asarray(lmbda)\n    Y = np.asarray(Y)\n    \n    # Ensure shape compatibility\n    if np.isscalar(lmbda):\n        lmbda = np.full_like(Y, lmbda, dtype=np.float64)\n    \n    # Compute log-likelihood\n    loglik = np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n    return loglik\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('blueprinty.csv')\nY = df['patents'].values  # observed count data\n# Evaluate log-likelihood over a range of lambda values\nlambda_values = np.linspace(0.1, 10, 200)  # avoid zero to prevent log(0)\nloglik_values = [poisson_loglikelihood(lmb, Y) for lmb in lambda_values]\n\n# Plotting\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_values, loglik_values, label='Log-Likelihood')\nplt.axvline(np.mean(Y), color='red', linestyle='--', label='Sample Mean (MLE)')\nplt.title('Poisson Log-Likelihood vs. Lambda')\nplt.xlabel('Lambda')\nplt.ylabel('Log-Likelihood')\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nTo estimate the Poisson rate parameter , we use Maximum Likelihood Estimation (MLE). The log-likelihood function for a sample y_1, y_2, , y_n drawn independently from a Poisson distribution is:\n\\[\n\\ell(\\lambda) = -n\\lambda + \\left(\\sum_{i=1}^n y_i\\right) \\log \\lambda - \\sum_{i=1}^n \\log(y_i!)\n\\]\nTo find the value of that maximizes this likelihood, we take the first derivative with respect to :\n\\[\n\\frac{\\partial \\ell}{\\partial \\lambda} = -n + \\frac{\\sum_{i=1}^n y_i}{\\lambda}\n\\]\nSetting the derivative equal to zero gives the critical point:\n\\[\n-n + \\frac{\\sum_{i=1}^n y_i}{\\lambda} = 0\n\\]\nSolving for , we obtain:\n\\[\n\\lambda = \\frac{\\sum_{i=1}^n y_i}{n}\n\\]\nThus, the maximum likelihood estimator (MLE) for is the sample mean, :\n\\[\n\\lambda_{\\text{MLE}} = \\overline{y}\n\\]\nThis result aligns with our intuition: in a Poisson distribution, the mean and variance are both equal to , so the best estimate for the average rate of occurrence is the observed average in the data.\n\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\n\n# Define the negative log-likelihood function\ndef neg_log_likelihood(lmbda, Y):\n    lmbda = lmbda[0]  # Extract scalar from array\n    if lmbda &lt;= 0:\n        return np.inf\n    return -np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n# Initial guess (sample mean)\ninitial_lambda = np.array([np.mean(Y)])\n# Perform the optimization\nresult = minimize(fun=neg_log_likelihood, x0=initial_lambda, args=(Y,), method='BFGS')\n# Extract results\nlambda_mle = result.x[0]\n# Print the MLE\nprint(f\"The MLE of lambda is: {lambda_mle:.4f}\")\n\nThe MLE of lambda is: 3.6847\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\ndef poisson_regression_neg_loglikelihood(beta, Y, X):\n    \"\"\"\n    Computes the negative log-likelihood for Poisson regression.\n    \n    Parameters:\n    - beta: array-like, shape (p,), model coefficients\n    - Y: array-like, shape (n,), observed counts\n    - X: array-like, shape (n, p), design matrix of covariates\n    \n    Returns:\n    - Negative log-likelihood (scalar)\n    \"\"\"\n    \n    beta = np.asarray(beta)\n    X = np.asarray(X)\n    Y = np.asarray(Y)    \n    # Compute lambda_i = exp(X_i^T * beta)\n    lambda_ = np.exp(X @ beta)    \n    # Compute log-likelihood\n    log_likelihood = np.sum(Y * np.log(lambda_) - lambda_ - gammaln(Y + 1))   \n    return -log_likelihood  \n\n\n\n         Current function value: nan\n         Iterations: 1\n         Function evaluations: 1008\n         Gradient evaluations: 112\n\nPoisson Regression Coefficients and Standard Errors:\n\n\n\n\n\n\n\n\n\nEstimate\nStdError\n\n\n\n\nIntercept\n1.4801\n1.0\n\n\nC(region)[T.Northeast]\n0.6410\n1.0\n\n\nC(region)[T.Northwest]\n0.1643\n1.0\n\n\nC(region)[T.South]\n0.1816\n1.0\n\n\nC(region)[T.Southwest]\n0.2955\n1.0\n\n\nage\n38.0164\n1.0\n\n\nI(age ** 2)\n1033.5396\n1.0\n\n\niscustomer\n0.5539\n1.0\n\n\n\n\n\n\n\n\nfrom IPython.display import display\n\n# Rename columns to match your desired output\nresults_df.columns = [\"Estimate\", \"Standard Error\"]\n\n# Display nicely with caption\ndisplay(results_df.style.set_caption(\"Estimated Coefficients and Standard Errors for Poisson Regression Model\")\n                        .set_properties(**{'text-align': 'center'}))\n\n\n\n\n\n\nTable 1: Estimated Coefficients and Standard Errors for Poisson Regression Model\n\n\n\n\n\n \nEstimate\nStandard Error\n\n\n\n\nIntercept\n1.480059\n1.000000\n\n\nC(region)[T.Northeast]\n0.640979\n1.000000\n\n\nC(region)[T.Northwest]\n0.164288\n1.000000\n\n\nC(region)[T.South]\n0.181562\n1.000000\n\n\nC(region)[T.Southwest]\n0.295497\n1.000000\n\n\nage\n38.016417\n1.000000\n\n\nI(age ** 2)\n1033.539585\n1.000000\n\n\niscustomer\n0.553874\n1.000000\n\n\n\n\n\n\n\n\nThe estimated coefficient for iscustomer is 0.5539, suggesting that firms using Blueprinty’s software have a higher expected number of patents. Since Poisson regression models the log of expected counts, we exponentiate the coefficient to interpret it on the original scale: e^{0.5539} \nThis implies that, holding age and region constant, Blueprinty customers are expected to receive 74% more patents than non-customers. However, the magnitude of the coefficients for age (38.02) and age² (1033.54) is unusually large and likely reflects scaling issues. This suggests the model may benefit from centering or standardizing the age variable. Finally, the fact that all standard errors are reported as 1.000 raises a concern: it’s likely that results_df was not constructed from actual model output and should be validated to ensure correct inference.\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                patents   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Mon, 05 May 2025   Deviance:                       2143.3\nTime:                        23:24:28   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nconst               -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage                  0.1486      0.014     10.716      0.000       0.121       0.176\nage_squared         -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer           0.2076      0.031      6.719      0.000       0.147       0.268\nregion_Northeast     0.0292      0.044      0.669      0.504      -0.056       0.115\nregion_Northwest    -0.0176      0.054     -0.327      0.744      -0.123       0.088\nregion_South         0.0566      0.053      1.074      0.283      -0.047       0.160\nregion_Southwest     0.0506      0.047      1.072      0.284      -0.042       0.143\n====================================================================================\n\n\nThe Poisson regression results provide strong evidence that using Blueprinty’s software is associated with increased patenting activity among engineering firms. Firms that use the software are expected to produce approximately 23% more patents over a five-year period, even after accounting for firm age and regional location. This finding supports the claim that Blueprinty’s software contributes positively to patent success.\n\n\n\n\n\n\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n[0.025\n0.975]\n\n\n\n\nconst\n-0.5089\n0.1832\n-2.7783\n0.0055\n-0.8679\n-0.1499\n\n\nage\n0.1486\n0.0139\n10.7162\n0.0000\n0.1214\n0.1758\n\n\nage_squared\n-0.0030\n0.0003\n-11.5132\n0.0000\n-0.0035\n-0.0025\n\n\niscustomer\n0.2076\n0.0309\n6.7192\n0.0000\n0.1470\n0.2681\n\n\nregion_Northeast\n0.0292\n0.0436\n0.6686\n0.5037\n-0.0563\n0.1147\n\n\nregion_Northwest\n-0.0176\n0.0538\n-0.3268\n0.7438\n-0.1230\n0.0878\n\n\nregion_South\n0.0566\n0.0527\n1.0740\n0.2828\n-0.0467\n0.1598\n\n\nregion_Southwest\n0.0506\n0.0472\n1.0716\n0.2839\n-0.0419\n0.1431\n\n\n\n\n\nThe Poisson regression analysis provides clear evidence that the use of Blueprinty’s software is significantly associated with higher patent output among engineering firms. The coefficient for iscustomer (0.2076, p &lt; 0.001) suggests that firms using Blueprinty’s tools are expected to produce approximately 23% more patents, holding other factors constant. This finding offers strong empirical support for the claim that Blueprinty’s software contributes to improved patenting outcomes.\nFirm age is also an important predictor. The positive coefficient for age and the negative coefficient for age squared indicate a nonlinear relationship: patent activity increases as firms mature but eventually slows down—reflecting a typical lifecycle pattern of innovation intensity.\nIn contrast, the analysis finds no statistically significant differences across regions, suggesting that regional location does not meaningfully influence patent success once firm-level factors are considered.\n\n\n\nOverall, these findings reinforce the value of Blueprinty’s software as a meaningful contributor to patent productivity. At the same time, they highlight the role of firm maturity in shaping innovation outcomes. While regional variation appears limited, the analysis underscores the importance of targeting firms at the right stage of development and aligning product value with their innovation capacity. These insights can guide Blueprinty’s strategic messaging and outreach efforts, particularly when engaging established firms seeking to strengthen their patent portfolios."
  },
  {
    "objectID": "hw2_questions.html#airbnb-case-study",
    "href": "hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nData\n\n\n          Unnamed: 0            id          days last_scraped  host_since  \\\ncount   30160.000000  3.016000e+04  30160.000000        30160       30140   \nunique           NaN           NaN           NaN            2        2738   \ntop              NaN           NaN           NaN     4/2/2017  12/21/2015   \nfreq             NaN           NaN           NaN        19141          56   \nmean    18679.352255  8.978287e+06   1139.711174          NaN         NaN   \nstd     11318.004370  5.376701e+06   1252.303675          NaN         NaN   \nmin         1.000000  2.515000e+03      7.000000          NaN         NaN   \n25%      8629.750000  4.276690e+06    584.000000          NaN         NaN   \n50%     18235.500000  9.149028e+06   1041.000000          NaN         NaN   \n75%     28532.250000  1.391476e+07   1592.000000          NaN         NaN   \nmax     40504.000000  1.797369e+07  42828.000000          NaN         NaN   \n\n              room_type     bathrooms      bedrooms         price  \\\ncount             30160  30160.000000  30160.000000  30160.000000   \nunique                3           NaN           NaN           NaN   \ntop     Entire home/apt           NaN           NaN           NaN   \nfreq              15543           NaN           NaN           NaN   \nmean                NaN      1.122132      1.151459    140.206863   \nstd                 NaN      0.384916      0.699010    188.392314   \nmin                 NaN      0.000000      0.000000     10.000000   \n25%                 NaN      1.000000      1.000000     70.000000   \n50%                 NaN      1.000000      1.000000    103.000000   \n75%                 NaN      1.000000      1.000000    169.000000   \nmax                 NaN      6.000000     10.000000  10000.000000   \n\n        number_of_reviews  review_scores_cleanliness  review_scores_location  \\\ncount        30160.000000               30160.000000            30160.000000   \nunique                NaN                        NaN                     NaN   \ntop                   NaN                        NaN                     NaN   \nfreq                  NaN                        NaN                     NaN   \nmean            21.170889                   9.201724                9.415351   \nstd             32.007541                   1.114261                0.843185   \nmin              1.000000                   2.000000                2.000000   \n25%              3.000000                   9.000000                9.000000   \n50%              8.000000                  10.000000               10.000000   \n75%             26.000000                  10.000000               10.000000   \nmax            421.000000                  10.000000               10.000000   \n\n        review_scores_value instant_bookable  \ncount          30160.000000            30160  \nunique                  NaN                2  \ntop                     NaN                f  \nfreq                    NaN            24243  \nmean               9.333952              NaN  \nstd                0.900472              NaN  \nmin                2.000000              NaN  \n25%                9.000000              NaN  \n50%               10.000000              NaN  \n75%               10.000000              NaN  \nmax               10.000000              NaN  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 30160 entries, 0 to 40503\nData columns (total 14 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   Unnamed: 0                 30160 non-null  int64  \n 1   id                         30160 non-null  int64  \n 2   days                       30160 non-null  int64  \n 3   last_scraped               30160 non-null  object \n 4   host_since                 30140 non-null  object \n 5   room_type                  30160 non-null  object \n 6   bathrooms                  30160 non-null  float64\n 7   bedrooms                   30160 non-null  float64\n 8   price                      30160 non-null  int64  \n 9   number_of_reviews          30160 non-null  int64  \n 10  review_scores_cleanliness  30160 non-null  float64\n 11  review_scores_location     30160 non-null  float64\n 12  review_scores_value        30160 non-null  float64\n 13  instant_bookable           30160 non-null  object \ndtypes: float64(5), int64(5), object(4)\nmemory usage: 3.5+ MB\nNone\n\n\n\n\nDescriptive\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\n# Distribution of Price\nplt.figure(figsize=(8, 5))\nsns.histplot(airbnb_data['price'], bins=30, kde=False)\nplt.title(\"Distribution of Prices\")\nplt.xlabel(\"Price\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# Distribution of Number of Reviews\nplt.figure(figsize=(8, 5))\nsns.histplot(airbnb_data['number_of_reviews'], bins=30)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# Distribution of Bedrooms\nplt.figure(figsize=(8, 5))\nsns.histplot(airbnb_data['bedrooms'], bins=10)\nplt.title(\"Distribution of Bedrooms\")\nplt.xlabel(\"Number of Bedrooms\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# Distribution of Bathrooms\nplt.figure(figsize=(8, 5))\nsns.histplot(airbnb_data['bathrooms'], bins=10)\nplt.title(\"Distribution of Bathrooms\")\nplt.xlabel(\"Number of Bathrooms\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 5))\nsns.scatterplot(data=airbnb_data, x='price', y='number_of_reviews', alpha=0.5, color='blue')\nplt.title(\"Price vs. Number of Reviews\")\nplt.xlabel(\"Price ($)\")\nplt.ylabel(\"Number of Reviews\")\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 5))\nsns.scatterplot(data=airbnb_data, x='bedrooms', y='number_of_reviews')\nplt.title(\"Bedrooms vs. Number of Reviews\")\nplt.xlabel(\"Bedrooms\")\nplt.ylabel(\"Number of Reviews\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Fit the Poisson regression model\npoisson_model = smf.glm(\n    formula=\"number_of_reviews ~ bedrooms + bathrooms + price + review_scores_cleanliness + review_scores_location + review_scores_value + instant_bookable\",\n    data=airbnb_data,\n    family=sm.families.Poisson()\n).fit()\n\n# Display the summary\nprint(poisson_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:      number_of_reviews   No. Observations:                30160\nModel:                            GLM   Df Residuals:                    30152\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -5.2946e+05\nDate:                Mon, 05 May 2025   Deviance:                   9.3745e+05\nTime:                        23:24:29   Pearson chi2:                 1.41e+06\nNo. Iterations:                     6   Pseudo R-squ. (CS):             0.5514\nCovariance Type:            nonrobust                                         \n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nIntercept                     3.5429      0.016    224.421      0.000       3.512       3.574\ninstant_bookable[T.t]         0.3319      0.003    115.214      0.000       0.326       0.337\nbedrooms                      0.0782      0.002     39.823      0.000       0.074       0.082\nbathrooms                    -0.1286      0.004    -34.532      0.000      -0.136      -0.121\nprice                      1.134e-05   7.41e-06      1.531      0.126   -3.18e-06    2.59e-05\nreview_scores_cleanliness     0.1135      0.001     76.217      0.000       0.111       0.116\nreview_scores_location       -0.0755      0.002    -47.159      0.000      -0.079      -0.072\nreview_scores_value          -0.0916      0.002    -51.114      0.000      -0.095      -0.088\n=============================================================================================\n\n\nThe Poisson regression model reveals key drivers of Airbnb review counts. The intercept (3.543) sets a baseline log count, though it’s mainly a mathematical reference point.\nBedrooms: Each additional bedroom increases expected reviews by ~8% (exp(0.0782)), suggesting that larger properties engage more guests.\nBathrooms: Surprisingly, more bathrooms reduce reviews by ~12% (exp(-0.1286)), which may reflect less demand or review activity in niche or high-end listings.\nPricing: Price has no statistically significant effect on review count (p = 0.126), indicating that, within this dataset, pricing is not a major engagement factor.\nGuest Ratings: Cleanliness significantly boosts reviews—each additional point corresponds to a ~12% increase, highlighting its importance. Location and Value scores are negatively associated with reviews, which may reflect confounding or lower guest motivation to review when expectations are fully met.\nBooking Convenience: Instant Bookable listings receive ~39% more reviews (exp(0.3319)), showing the value of booking ease in driving guest interaction.\nModel Fit: While the model fits reasonably well (Pseudo R² = 0.55), the high Pearson chi-squared suggests potential overdispersion—worth addressing with a Negative Binomial model.\n\n# Fit Negative Binomial regression model\nnb_model = smf.glm(\n    formula=\"number_of_reviews ~ bedrooms + bathrooms + price + review_scores_cleanliness + review_scores_location + review_scores_value + instant_bookable\",\n    data=airbnb_data,\n    family=sm.families.NegativeBinomial()\n).fit()\n\n# Display summary\nprint(nb_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:      number_of_reviews   No. Observations:                30160\nModel:                            GLM   Df Residuals:                    30152\nModel Family:        NegativeBinomial   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -1.2225e+05\nDate:                Mon, 05 May 2025   Deviance:                       47959.\nTime:                        23:24:29   Pearson chi2:                 6.49e+04\nNo. Iterations:                    21   Pseudo R-squ. (CS):            0.04422\nCovariance Type:            nonrobust                                         \n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nIntercept                     4.1717      0.078     53.149      0.000       4.018       4.326\ninstant_bookable[T.t]         0.3263      0.015     21.992      0.000       0.297       0.355\nbedrooms                      0.0741      0.009      7.819      0.000       0.056       0.093\nbathrooms                    -0.1142      0.017     -6.703      0.000      -0.148      -0.081\nprice                      6.489e-06   3.36e-05      0.193      0.847   -5.94e-05    7.24e-05\nreview_scores_cleanliness     0.1961      0.007     28.891      0.000       0.183       0.209\nreview_scores_location       -0.1084      0.008    -13.702      0.000      -0.124      -0.093\nreview_scores_value          -0.2087      0.009    -23.657      0.000      -0.226      -0.191\n=============================================================================================\n\n\n/Users/qqtweety/Downloads/mgta495/.venv/lib/python3.9/site-packages/statsmodels/genmod/families/family.py:1367: ValueWarning: Negative binomial dispersion parameter alpha not set. Using default value alpha=1.0.\n  warnings.warn(\"Negative binomial dispersion parameter alpha not \"\n\n\nThe Negative Binomial regression model reveals clear patterns in what drives Airbnb review counts, addressing overdispersion present in the Poisson model. The model confirms that listing features and guest experience ratings significantly influence engagement.\nKey Drivers: Bedrooms: Each additional bedroom increases expected reviews by ~7.7% (exp(0.0741)), likely reflecting higher capacity and group travel.\nBathrooms: More bathrooms are associated with fewer reviews (~10.8% decrease), though the reason is unclear and may relate to unobserved listing characteristics.\nPrice: Has no significant effect, suggesting review likelihood is not sensitive to rental cost within the dataset’s range.\nGuest Experience Ratings: Cleanliness: Strongly predictive—each point increase leads to ~21.7% more reviews, highlighting its role in satisfaction.\nLocation & Value: Unexpectedly, higher scores correlate with fewer reviews. This may reflect a lower urgency to leave feedback when expectations are met.\nInstant Bookability:Listings with this feature see ~38.5% more reviews (exp(0.3263)), underlining the value of booking convenience.\nModel Fit: The model’s lower deviance and log-likelihood indicate improved fit over Poisson, and confirm the presence of overdispersion. Still, a low pseudo R² (0.044) suggests many unobserved factors influence reviews.\n\n\nConclusion\nIn conclusion, this analysis provides actionable insights for Airbnb hosts seeking to increase guest reviews. Practical features—such as offering more bedrooms and enabling instant booking—are associated with higher review counts, likely due to their appeal to larger groups and convenience-focused travelers. Cleanliness stands out as the most influential factor, reinforcing its critical role in guest satisfaction and review likelihood.\nWhile review scores for location and value also play a role, their effects are more nuanced and may reflect complex guest expectations. These findings suggest that while enhancing key amenities is important, understanding guest behavior remains an area for further exploration. Overall, hosts can benefit by focusing on what matters most: delivering clean, accessible, and well-equipped stays that encourage positive engagement."
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse."
  },
  {
    "objectID": "hw1_questions.html#objective",
    "href": "hw1_questions.html#objective",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Objective",
    "text": "Objective\nThe objective of this assignment is to replicate and interpret the findings from Karlan and List’s 2007 field experiment on charitable giving. Using the original dataset, I aim to explore how the presence and structure of a matching donation offer affects both the likelihood and amount of charitable contributions.\nThis project involves: • Analyzing treatment effects using t-tests, regression models, and probit analysis • Comparing donation behavior across different match ratios and suggested ask levels • Conducting simulations to illustrate the Law of Large Numbers and the Central Limit Theorem • Presenting results in a clear, reproducible format using Quarto and Python\nThe overall goal is to better understand the behavioral response to charitable incentives and how small changes in message framing can impact donor behavior."
  },
  {
    "objectID": "hw1_questions.html#description-of-the-experiment",
    "href": "hw1_questions.html#description-of-the-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Description of the Experiment",
    "text": "Description of the Experiment\nIn their 2007 study published in the American Economic Review, Dean Karlan and John List conducted a large-scale natural field experiment to test the effectiveness of matching donations in a real-world fundraising campaign. Over 50,000 previous donors to a U.S. nonprofit were mailed fundraising letters and randomly assigned to one of two groups: • The control group received a standard fundraising letter with no special offer. • The treatment group received a similar letter but was offered a matching donation — meaning their contribution would be matched by another donor.\nWithin the treatment group, individuals were further randomized into subgroups based on: • Match ratio: $1:$1, $2:$1, or $3:$1 • Maximum match amount: $25,000, $50,000, $100,000, or unstated • Suggested donation amount: equal to, 1.25×, or 1.5× the donor’s previous contribution\nThe randomized design allows for a causal analysis of how these variations influence both the decision to donate and the amount given. This experiment provides a powerful example of how field experiments can be used to study economic behavior in natural settings"
  },
  {
    "objectID": "hw1_questions.html#data-description",
    "href": "hw1_questions.html#data-description",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data Description",
    "text": "Data Description\nWe begin by loading the dataset provided by Karlan and List (2007), which contains detailed records from their fundraising field experiment.\n\nimport pandas as pd\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n# Basic treatment/control breakdown\nprint(\"Unique match ratios:\", df[\"ratio\"].unique())\nprint(\"Treatment group size:\", df[df[\"treatment\"] == 1].shape[0])\nprint(\"Control group size:\", df[df[\"treatment\"] == 0].shape[0])\n\nUnique match ratios: ['Control', 1, 2, 3]\nCategories (4, object): ['Control' &lt; 1 &lt; 2 &lt; 3]\nTreatment group size: 33396\nControl group size: 16687\n\n\n\n# Summary statistics: mean donation rate and average donation by group\nsummary = df.groupby(\"treatment\")[[\"gave\", \"amount\"]].agg(['mean', 'count']).round(4)\nsummary\n\n\n\n\n\n\n\n\ngave\namount\n\n\n\nmean\ncount\nmean\ncount\n\n\ntreatment\n\n\n\n\n\n\n\n\n0\n0.0179\n16687\n0.8133\n16687\n\n\n1\n0.0220\n33396\n0.9669\n33396"
  },
  {
    "objectID": "hw1_questions.html#balance-test",
    "href": "hw1_questions.html#balance-test",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Balance Test",
    "text": "Balance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nT-Test and Linear Regression for Each Variable\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\ndef balance_test(var):\n    subset = df[['treatment', var]].dropna()\n    \n    # T-test\n    treated = subset[subset['treatment'] == 1][var]\n    control = subset[subset['treatment'] == 0][var]\n    t_stat, p_val = ttest_ind(treated, control, equal_var=False)\n    print(f\"\\nT-test for {var}: t = {t_stat:.3f}, p = {p_val:.4f}\")\n    \n    # Linear regression\n    model = smf.ols(f'{var} ~ treatment', data=subset).fit()\n    print(model.summary().tables[1])  # Coefficient table\n\n# Run for selected baseline variables\nfor var in ['mrm2', 'years', 'freq', 'female']:\n    balance_test(var)\n\n\nT-test for mrm2: t = 0.120, p = 0.9049\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\n\nT-test for years: t = -1.091, p = 0.2753\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      6.1359      0.043    144.023      0.000       6.052       6.219\ntreatment     -0.0575      0.052     -1.103      0.270      -0.160       0.045\n==============================================================================\n\nT-test for freq: t = -0.111, p = 0.9117\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      8.0473      0.088     91.231      0.000       7.874       8.220\ntreatment     -0.0120      0.108     -0.111      0.912      -0.224       0.200\n==============================================================================\n\nT-test for female: t = -1.754, p = 0.0795\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.2827      0.004     80.688      0.000       0.276       0.290\ntreatment     -0.0075      0.004     -1.758      0.079      -0.016       0.001\n==============================================================================\n\n\nTo evaluate whether the randomization process produced balanced groups, we compared key pre-treatment characteristics between the treatment and control groups using both t-tests and bivariate linear regressions. The variables examined included:\n• mrm2 (months since last donation)\n• years (since first donation)\n• freq (number of past donations)\n• female (gender)\nIn all cases, the p-values from both methods exceeded 0.05, indicating that none of the differences were statistically significant. This means the treatment and control groups were well-balanced across these characteristics, just as Table 1 in Karlan & List (2007) demonstrates.\nThis validation is crucial for the study’s internal validity, it confirms that differences in outcomes can be attributed to the treatment, not to pre-existing differences in donor behavior or demographics.\n\n\nT-Test and Regression for mrm2\n\n# Drop NA for mrm2\nbalance_df = df[['treatment', 'mrm2']].dropna()\n\n# T-TEST \ntreated = balance_df[balance_df['treatment'] == 1]['mrm2']\ncontrol = balance_df[balance_df['treatment'] == 0]['mrm2']\nt_stat, p_val = ttest_ind(treated, control, equal_var=False)\nprint(f\"T-test for mrm2: t = {t_stat:.3f}, p = {p_val:.4f}\")\n# LINEAR REGRESSION \nmodel = smf.ols('mrm2 ~ treatment', data=balance_df).fit()\nprint(model.summary().tables[1])  # Show only coefficient table\n\nT-test for mrm2: t = 0.120, p = 0.9049\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\n\n\nTo assess whether the treatment and control groups were statistically different before the intervention, we performed a balance test on the variable mrm2, which measures the number of months since the last donation.\nA t-test comparing the mean mrm2 between groups yielded a t-statistic of 0.120 and a p-value of 0.9049, indicating no statistically significant difference between the groups.\nWe also ran a linear regression of mrm2 on the treatment indicator. The estimated coefficient on treatment was 0.0137, with a p-value of 0.905 and a 95% confidence interval ranging from −0.211 to 0.238. These results confirm the same conclusion: the treatment group and control group had nearly identical values for this variable prior to the intervention.\nThis finding supports the integrity of the random assignment in the experiment. Since there are no significant differences in months since last donation, we can be confident that any observed differences in donation behavior after treatment are not due to baseline imbalances. This type of balance check is crucial for ensuring the internal validity of experimental results and mirrors the intent of Table 1 in Karlan & List (2007).\n\n\nRepeat for More Variables\n\ndef run_balance_test(var):\n    subset = df[['treatment', var]].dropna()\n    \n    # T-test\n    treated = subset[subset['treatment'] == 1][var]\n    control = subset[subset['treatment'] == 0][var]\n    t_stat, p_val = ttest_ind(treated, control, equal_var=False)\n    print(f\"\\nT-test for {var}: t = {t_stat:.3f}, p = {p_val:.4f}\")\n    \n    # Regression\n    model = smf.ols(f'{var} ~ treatment', data=subset).fit()\n    print(model.summary().tables[1])  # Coefficient summary only\n\n# Run tests for mrm2, years, freq, and female\nfor variable in ['mrm2', 'years', 'freq', 'female']:\n    run_balance_test(variable)\n\n\nT-test for mrm2: t = 0.120, p = 0.9049\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\n\nT-test for years: t = -1.091, p = 0.2753\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      6.1359      0.043    144.023      0.000       6.052       6.219\ntreatment     -0.0575      0.052     -1.103      0.270      -0.160       0.045\n==============================================================================\n\nT-test for freq: t = -0.111, p = 0.9117\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      8.0473      0.088     91.231      0.000       7.874       8.220\ntreatment     -0.0120      0.108     -0.111      0.912      -0.224       0.200\n==============================================================================\n\nT-test for female: t = -1.754, p = 0.0795\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.2827      0.004     80.688      0.000       0.276       0.290\ntreatment     -0.0075      0.004     -1.758      0.079      -0.016       0.001\n==============================================================================\n\n\nTo verify the success of the randomization procedure, we conducted balance tests on four key pre-treatment variables:\n• mrm2 (months since last donation)\n• years (years since first donation)\n• freq (number of prior donations)\n• female (gender indicator)\nFor each variable, we conducted both a t-test and a bivariate linear regression of the variable on the treatment indicator. The results from both methods were consistent and are summarized below:\n• mrm2: The t-test returned a p-value of 0.905, and the regression coefficient on treatment was 0.0137. There is no evidence of a difference between groups on months since last donation.\n• years: The p-value was 0.275 in the t-test and 0.270 in the regression, with a treatment coefficient of −0.0575. Again, no significant difference.\n• freq: The p-value was 0.912 and the treatment coefficient was −0.0120, indicating complete balance in the number of prior donations.\n• female: This variable had a slightly lower p-value of 0.079 and a treatment coefficient of −0.0075. While not statistically significant at the 5% level, it is closer to the threshold, suggesting a small imbalance that is worth noting, though likely not practically important.\nOverall, these tests show that treatment and control groups were well-balanced across observable characteristics prior to the intervention. These results validate the success of the randomization procedure, ensuring that post-treatment differences in outcomes can be interpreted as causal effects of the matching donation treatment. This approach follows the same logic as Table 1 in Karlan & List (2007), which is used to demonstrate the internal validity of their experimental design."
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\nBarplot of Donation Rate (Treatment vs Control)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Calculate response rate\ndonation_rate = df.groupby(\"treatment\")[\"gave\"].mean().reset_index()\ndonation_rate[\"Group\"] = donation_rate[\"treatment\"].map({0: \"Control\", 1: \"Treatment\"})\n\n# Plot\nplt.figure(figsize=(6,4))\nsns.barplot(data=donation_rate, x=\"Group\", y=\"gave\", errorbar=None)\nplt.title(\"Donation Rate by Treatment Status\")\nplt.ylabel(\"Proportion Who Donated\")\nplt.xlabel(\"Group\")\nplt.ylim(0, 0.03)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe barplot above compares the proportion of individuals who donated in the treatment group (offered a matching donation) and the control group (not offered a match).\n• The control group had a donation rate of approximately 1.8%\n• The treatment group had a slightly higher donation rate of approximately 2.2%\nThis visual illustrates a modest but meaningful difference in response rates. The treatment group was more likely to donate, consistent with the results from the t-test and regression analysis, both of which showed that this difference is statistically significant at the 1% level.\nWhile the absolute increase of around 0.4 percentage points may seem small, it represents a 22% relative increase in donation likelihood, a substantial behavioral shift from a simple intervention. This reinforces the broader conclusion that framing a donation as being matched can increase participation, even without changing the donation amount given.\n\n\nTreatment Effect on Donation Rates\n\n# T-TEST \ncontrol = df[df['treatment'] == 0]['gave']\ntreatment = df[df['treatment'] == 1]['gave']\nt_stat, p_val = ttest_ind(treatment, control, equal_var=False)\nprint(f\"T-test result: t = {t_stat:.3f}, p = {p_val:.4f}\")\n# BIVARIATE REGRESSION \nmodel = smf.ols('gave ~ treatment', data=df).fit()\nprint(model.summary().tables[1])  # Show coefficient table\n\nT-test result: t = 3.209, p = 0.0013\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\n\n\nTo determine whether the matching donation treatment increased the probability of giving, we compared the donation rates of the treatment and control groups using both a t-test and a bivariate linear regression.\nThe t-test found a statistically significant difference in donation rates (t = 3.209, p = 0.0013), indicating that the treatment and control groups do not have the same underlying probability of donating.\nThe regression model estimates that being assigned to the treatment group increases the probability of giving by 0.42 percentage points (coef = 0.0042, p = 0.002), and this effect is statistically significant at the 1% level.\nThe intercept of the model is 0.0179, which represents the average donation rate in the control group (1.79%). The treatment effect raises this to approximately 2.21%, closely matching the observed values in the raw data.\nThese results confirm that the difference in donation rates between the treatment and control groups is not due to random chance. The findings suggest that even a small behavioral nudge.For example being told a donation will be matched, can have a statistically significant impact on charitable behavior. While the absolute increase is modest, the result is highly robust and consistent across methods.\n\n\nProbit Regression\n\nimport statsmodels.api as sm\n# Prepare the data\nprobit_df = df[['gave', 'treatment']].dropna()\nX = sm.add_constant(probit_df['treatment'])  # add intercept\ny = probit_df['gave']\n# Run Probit model\nprobit_model = sm.Probit(y, X).fit()\nprint(probit_model.summary())\n\n# Compute average marginal effects (AME)\nmarginal_effects = probit_model.get_margeff(at='overall')\nprint(marginal_effects.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Mon, 05 May 2025   Pseudo R-squ.:               0.0009783\nTime:                        23:24:33   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\nWe used a Probit regression to estimate the effect of being assigned to the treatment group (offered a matching donation) on the likelihood of making a donation. The treatment coefficient was 0.0868 (p = 0.002), indicating a positive and statistically significant effect.\nTo interpret the impact in real terms, we calculated the average marginal effect, which was 0.0043, meaning the treatment increased the probability of donating by 0.43 percentage points.\nThe model’s log-likelihood improved from −5035.4 to −5030.5, and the likelihood ratio test confirmed the treatment variable significantly improved model fit (p = 0.0017)."
  },
  {
    "objectID": "hw1_questions.html#differences-between-match-rates",
    "href": "hw1_questions.html#differences-between-match-rates",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Differences between Match Rates",
    "text": "Differences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nDifferent match ratios (1:1 vs 2:1 vs 3:1) lead to different donation rates\n\nfrom scipy.stats import ttest_ind\n# Filter treatment group only\ntreat_only = df[df['treatment'] == 1].copy()\n# Ensure ratio is numeric\ntreat_only['ratio'] = pd.to_numeric(treat_only['ratio'], errors='coerce')\n# Extract 'gave' for each group\ngave_r1 = treat_only[treat_only['ratio'] == 1]['gave']\ngave_r2 = treat_only[treat_only['ratio'] == 2]['gave']\ngave_r3 = treat_only[treat_only['ratio'] == 3]['gave']\n# Run t-tests\n_, pval_12 = ttest_ind(gave_r1, gave_r2, equal_var=False)\n_, pval_13 = ttest_ind(gave_r1, gave_r3, equal_var=False)\n\nprint(f\"p-value (2:1 vs 1:1): {pval_12:.4f}\")\nprint(f\"p-value (3:1 vs 1:1): {pval_13:.4f}\")\n\np-value (2:1 vs 1:1): 0.3345\np-value (3:1 vs 1:1): 0.3101\n\n\nWe tested whether offering larger match ratios (2:1 or 3:1) influenced donation rates compared to a standard 1:1 match, using a series of t-tests.\nThe results showed:\n• A 0.19 percentage point increase in donation rate from 1:1 to 2:1, with a p-value of 0.3345\n• A 0.01 percentage point increase from 2:1 to 3:1, with a p-value of 0.3101\nThese p-values are well above 0.05, indicating that the differences are not statistically significant. In other words, we cannot conclude that higher match ratios led to meaningfully higher response rates.\nThese findings support the interpretation in Karlan & List (2007): although larger match ratios may look more appealing, they do not result in a significantly greater number of donors. The 1:1 match appears to be just as effective as the more generous 2:1 and 3:1 offers.\nThis suggests that the presence of a match offer matters more than the generosity of the match ratio, which has important practical implications for fundraisers aiming to design cost-effective campaigns.\n\n\nWhether match ratio affects donation behavior\n\nimport statsmodels.formula.api as smf\n# Filter treatment group only\ntreat_df = df[df['treatment'] == 1].copy()\n# Run regression: 1:1 is the omitted category\nmodel = smf.ols('gave ~ ratio2 + ratio3', data=treat_df).fit()\nprint(model.summary().tables[1])  # Show coefficient summary\n\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0207      0.001     14.912      0.000       0.018       0.023\nratio2         0.0019      0.002      0.958      0.338      -0.002       0.006\nratio3         0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\n\n\nTo assess whether higher match ratios influenced the likelihood of donating, we estimated a linear regression where the outcome was whether someone donated (gave), and the predictors were two dummy variables:\n\nratio2: equals 1 for the 2:1 match group\nratio3: equals 1 for the 3:1 match group\n\nThe 1:1 match group serves as the baseline category. The regression results are:\n\nThe intercept (0.0207) represents the donation rate under the 1:1 match.\nThe coefficient on ratio2 (0.0019) indicates a 0.19 percentage point increase in donation rate relative to 1:1, but this effect is not statistically significant (p = 0.338).\nThe coefficient on ratio3 (0.0020) indicates a 0.20 percentage point increase relative to 1:1, also not significant (p = 0.313).\n\nThese results suggest that larger match ratios (2:1 or 3:1) do not lead to a statistically meaningful increase in donation rates compared to the 1:1 match. The 95% confidence intervals for both ratio2 and ratio3 include zero, reinforcing that the observed differences could easily be due to chance.\nThis finding supports what Karlan & List (2007) concluded: the presence of a match offer increases donation rates, but increasing the match ratio beyond 1:1 does not substantially improve effectiveness. For fundraisers, this implies that a simple match offer may be just as effective as a more generous one, and likely more cost-efficient.\n\n\nDifferences in donation response rates between match ratios\n\n# Filter treatment group only\ntreat_df = df[df['treatment'] == 1].copy()\n# Convert ratio to numeric\ntreat_df['ratio'] = pd.to_numeric(treat_df['ratio'], errors='coerce')\n# Calculate mean donation rate for each match ratio\nresponse_rates = treat_df.groupby('ratio')['gave'].mean().round(4)\nprint(\"Average Donation Rates by Match Ratio:\")\nprint(response_rates)\n# Calculate differences\ndiff_2_vs_1 = response_rates[2] - response_rates[1]\ndiff_3_vs_2 = response_rates[3] - response_rates[2]\n\nprint(f\"\\nDifference (2:1 vs 1:1): {diff_2_vs_1:.4f}\")\nprint(f\"Difference (3:1 vs 2:1): {diff_3_vs_2:.4f}\")\n\nAverage Donation Rates by Match Ratio:\nratio\n1    0.0207\n2    0.0226\n3    0.0227\nName: gave, dtype: float64\n\nDifference (2:1 vs 1:1): 0.0019\nDifference (3:1 vs 2:1): 0.0001\n\n\nWe examined whether larger matching ratios led to higher response rates by comparing the proportion of individuals who donated under each match condition:\n• 1:1 match: 2.07% response rate\n• 2:1 match: 2.26% response rate\n• 3:1 match: 2.27% response rate\nThe increase in donation rate from 1:1 to 2:1 was just 0.19 percentage points, and from 2:1 to 3:1 the increase was an almost negligible 0.01 percentage points.\nThese differences are small in magnitude and not statistically significant, as confirmed by earlier t-tests and regression results. This suggests that offering a larger match ratio does not meaningfully boost participation compared to a simple 1:1 match.\nThese findings support what Karlan & List (2007) described in their paper, that although fundraisers often believe larger match ratios will lead to more giving, the data show that the presence of a match matters more than the specific size. Once a match is introduced, increasing the match multiplier offers little to no additional benefit in terms of motivating donors to give."
  },
  {
    "objectID": "hw1_questions.html#size-of-charitable-contribution",
    "href": "hw1_questions.html#size-of-charitable-contribution",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Size of Charitable Contribution",
    "text": "Size of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nTreatment affected how much people donated\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n# Prepare data\ncontrol_amt = df[df['treatment'] == 0]['amount']\ntreatment_amt = df[df['treatment'] == 1]['amount']\n# T-TEST \nt_stat, p_val = ttest_ind(treatment_amt, control_amt, equal_var=False)\nprint(f\"T-test result: t = {t_stat:.3f}, p = {p_val:.4f}\")\n\n#  REGRESSION \nmodel = smf.ols('amount ~ treatment', data=df).fit()\nprint(model.summary().tables[1])\n\nT-test result: t = 1.918, p = 0.0551\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\n\n\nWe analyzed whether individuals in the treatment group gave more or less than those in the control group among those who donated. This isolates the intensive margin of behavior, how generous donors were, as opposed to whether they chose to give at all.\nA t-test comparing donation amounts between treatment and control donors produced a non-significant result (t = -0.585, p = 0.5590). This indicates no statistically meaningful difference in average donation size between the two groups.\nThe regression analysis confirms this. The estimated treatment effect is −$1.67, meaning that donors in the treatment group gave slightly less on average than those in the control group. However, the p-value of 0.561 is far from any conventional threshold for significance, and the confidence interval (−$7.31 to +$3.97) includes zero, suggesting we cannot rule out the possibility that the true difference is zero.\nThese findings imply that while the treatment was effective in increasing the number of people who donated (extensive margin), it had no measurable impact on how much people gave once they decided to contribute (intensive margin).\n\n\nTreatment changed how much they gave\n\n# Subset to donors only (gave == 1)\ndonors_only = df[df['gave'] == 1].copy()\n# T-TEST \ntreatment_amt = donors_only[donors_only['treatment'] == 1]['amount']\ncontrol_amt = donors_only[donors_only['treatment'] == 0]['amount']\nt_stat, p_val = ttest_ind(treatment_amt, control_amt, equal_var=False)\nprint(f\"T-test (donors only): t = {t_stat:.3f}, p = {p_val:.4f}\")\n\n# REGRESSION \nmodel = smf.ols('amount ~ treatment', data=donors_only).fit()\nprint(model.summary().tables[1])\n\nT-test (donors only): t = -0.585, p = 0.5590\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\n\n\nWe restricted the analysis to only those who made a donation and tested whether the treatment affected how much they gave.\nA t-test showed no statistically significant difference in average donation amounts between treatment and control donors (p = 0.559). A linear regression confirms this finding: the estimated treatment effect is −$1.67, but this is not statistically significant (p = 0.561).\nThe control group gave an average of $45.54, while the treatment group gave slightly less, but the difference could easily be due to random variation.\nIt’s important to note that this regression does not have a clear causal interpretation, because we’re conditioning on a post-treatment outcome (gave == 1). The treatment likely influenced who chose to donate, and that selection can bias the estimate of how much people give, conditional on donating.\nIn summary, the treatment increased the number of donors, but not the average amount given by those who donated.\n\n\nDonation Amounts Among Donors by Group\n\n# Filter to donors only\ndonors = df[df['gave'] == 1]\n# Split by group\ncontrol_donors = donors[donors['treatment'] == 0]['amount']\ntreatment_donors = donors[donors['treatment'] == 1]['amount']\n# Calculate means\ncontrol_mean = control_donors.mean()\ntreatment_mean = treatment_donors.mean()\n# Create plots\nfig, axs = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Control histogram\naxs[0].hist(control_donors, bins=30, color='skyblue', edgecolor='black')\naxs[0].axvline(control_mean, color='red', linestyle='--', label=f'Mean = ${control_mean:.2f}')\naxs[0].set_title(\"Control Group: Donation Amounts\")\naxs[0].set_xlabel(\"Donation Amount\")\naxs[0].set_ylabel(\"Frequency\")\naxs[0].legend()\n\n# Treatment histogram\naxs[1].hist(treatment_donors, bins=30, color='lightgreen', edgecolor='black')\naxs[1].axvline(treatment_mean, color='red', linestyle='--', label=f'Mean = ${treatment_mean:.2f}')\naxs[1].set_title(\"Treatment Group: Donation Amounts\")\naxs[1].set_xlabel(\"Donation Amount\")\naxs[1].legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe two histograms display the distribution of donation amounts among those who donated, separated by control (left) and treatment (right) groups. Each plot includes a red dashed line indicating the mean donation amount for that group.\nIn the control group, the average donation is $45.54, and most donations cluster below $100, though a few large gifts extend beyond $250.\nIn the treatment group, the average donation is slightly lower at $43.87, with a very similar distribution shape. Most donations are small, and the long right tail reflects a few large contributions.\nThese visualizations reinforce the earlier statistical findings: although the treatment increased the number of people who gave, it did not significantly affect how much people gave, once they decided to donate. The similarity in shapes and overlapping means suggest that the match offer encouraged more people to participate, but didn’t influence the generosity of individual donors. This aligns with the idea that the treatment affects the extensive margin (whether people give), not the intensive margin (how much they give). ——"
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nSimulating Donation Outcomes with Bernoulli Distributions\n\nimport numpy as np\n\n# Set simulation parameters\np_control = 0.018\np_treatment = 0.022\nn = 500  # Sample size per group per iteration\niterations = 10000\nnp.random.seed(42)\n\n# Store simulated differences\ndiffs = []\n\nfor _ in range(iterations):\n    control_sample = np.random.binomial(1, p_control, size=n)\n    treatment_sample = np.random.binomial(1, p_treatment, size=n)\n    \n    diff = treatment_sample.mean() - control_sample.mean()\n    diffs.append(diff)\n\n# Convert to array\ndiffs = np.array(diffs)\n\n# Plot distribution of simulated differences\nplt.figure(figsize=(10, 5))\nplt.hist(diffs, bins=40, color='lightsteelblue', edgecolor='black')\nplt.axvline(x=np.mean(diffs), color='red', linestyle='--', label=f'Mean Diff = {np.mean(diffs):.4f}')\nplt.axvline(x=0, color='black', linestyle=':', label='Null (No Effect)')\nplt.title('Sampling Distribution of Treatment-Control Differences (Simulated)')\nplt.xlabel('Difference in Proportion Donating')\nplt.ylabel('Frequency')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe simulate 10,000 experiments in which we draw samples of size 500 from a control group (p = 0.018) and a treatment group (p = 0.022), both assumed to follow Bernoulli distributions. In each iteration, we calculate the difference in donation rates between the two groups.\nThe resulting distribution of differences demonstrates the Central Limit Theorem: even though the underlying data are binary, the distribution of differences is approximately normal and centered near the true effect size (0.004).\nThis also illustrates the Law of Large Numbers: with enough repetitions, the sample average of these differences converges to the expected value. The simulation confirms that the observed effect in our real data (≈ 0.0043) is consistent with what we’d expect if the true probabilities were 1.8% and 2.2%."
  },
  {
    "objectID": "hw1_questions.html#law-of-large-numbers",
    "href": "hw1_questions.html#law-of-large-numbers",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers\n\nSimulate and Plot Cumulative Average of Differences\n\n# Sample real data without replacement\ndf_shuffled = df[['treatment', 'amount']].sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Encode treatment as +1 or -1 to compute difference\ndf_shuffled['diff'] = df_shuffled['amount'] * df_shuffled['treatment'] - df_shuffled['amount'] * (1 - df_shuffled['treatment'])\n\n# Cumulative average difference (treatment - control contribution)\ncumulative_avg = np.cumsum(df_shuffled['diff']) / np.arange(1, len(df_shuffled) + 1)\n\n# True difference from earlier\ntrue_diff = df[df['treatment'] == 1]['amount'].mean() - df[df['treatment'] == 0]['amount'].mean()\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Avg (Shuffled Real Data)')\nplt.axhline(y=true_diff, color='red', linestyle='--', label=f'True Difference ≈ {true_diff:.4f}')\nplt.title(\"Cumulative Average Difference\")\nplt.xlabel(\"Observation Index\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis plot displays the cumulative average difference in donation amounts (treatment minus control) based on the real dataset, where observations were randomly shuffled. Each point on the line represents the average difference after including one more data point.\nAt the beginning, the average fluctuates due to the small number of observations. As more data are added, the line smooths out and stabilizes near the true treatment effect (shown by the red dashed line).\nThis visually demonstrates the Law of Large Numbers: as the number of observations increases, the sample average converges to the true population difference, providing strong support that the observed treatment effect is consistent and not due to chance."
  },
  {
    "objectID": "hw1_questions.html#central-limit-theorem",
    "href": "hw1_questions.html#central-limit-theorem",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nSimulation Histograms (Sample Sizes 50, 200, 500, 1000)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Use observed donation behavior (binary) from your dataset\np_control = 0.018  # Control group's donation probability\np_treatment = 0.022  # Treatment group's donation probability\n\n# Sample sizes to simulate\nsample_sizes = [50, 200, 500, 1000]\nsimulations = 1000  # Number of experiments per sample size\n\n# Set up plot grid\nfig, axs = plt.subplots(2, 2, figsize=(14, 10))\naxs = axs.flatten()\n\n# Run simulations and plot\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n    for _ in range(simulations):\n        control_sample = np.random.binomial(1, p_control, size=n)\n        treatment_sample = np.random.binomial(1, p_treatment, size=n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        diffs.append(diff)\n\n    mean_diff = np.mean(diffs)\n    \n    # Plot histogram\n    axs[i].hist(diffs, bins=30, color='lightblue', edgecolor='black')\n    axs[i].axvline(x=0, color='red', linestyle='--', label='Zero')\n    axs[i].axvline(x=mean_diff, color='green', linestyle='-', label='Mean')\n    axs[i].set_title(f\"Sample Size = {n}\")\n    axs[i].set_xlabel(\"Average Difference (Treatment − Control)\")\n    axs[i].set_ylabel(\"Frequency\")\n    axs[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe four histograms illustrate the sampling distributions of the average difference in donation rates between the treatment and control groups, calculated from 1,000 simulated experiments at sample sizes of 50, 200, 500, and 1,000.\nIn each simulation, we randomly sampled from Bernoulli distributions (with p = 0.022 for treatment and p = 0.018 for control) and computed the average difference in donation rates. The histograms show the distribution of those average differences across simulations.\nSample Size = 50: The distribution is wide and irregular, showing high variability. The mean (green line) and the null hypothesis value (zero, red dashed line) are close together, and the effect is difficult to distinguish from noise.\nSample Size = 200: The distribution is smoother and narrower. The mean shifts slightly to the right, showing a small positive treatment effect. Zero is still near the center, but the result starts to show more separation.\nSample Size = 500: The distribution becomes clearly bell-shaped and narrower. The mean moves further away from zero, and now zero lies in the tail of the distribution, suggesting the treatment effect is more detectable.\nSample Size = 1000: The distribution is even tighter, with the average difference centered around the true effect. The null value (zero) is well into the tail, meaning that if this were real experimental data, the treatment effect would likely be statistically significant.\nThese plots demonstrate both the Central Limit Theorem (distributions become normal as n increases) and the Law of Large Numbers (sample averages converge to true values). They also show how larger sample sizes increase the reliability of statistical estimates and reduce the likelihood of failing to detect real effects."
  }
]