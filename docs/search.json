[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tweety Chang",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse."
  },
  {
    "objectID": "hw1_questions.html#description-of-the-experiment",
    "href": "hw1_questions.html#description-of-the-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Description of the Experiment",
    "text": "Description of the Experiment\nIn their 2007 paper published in the American Economic Review, Karlan and List conducted a large-scale natural field experiment to investigate how matching donations affect charitable giving. They sent over 50,000 direct mail solicitations to previous donors of a U.S. nonprofit organization. Individuals were randomly assigned to either a control group, which received a standard fundraising appeal, or a treatment group, which received a matching donation offer.\nWithin the treatment group, participants were further randomized into subgroups based on: - Match ratio: $1:$1, $2:$1, or $3:$1 - Maximum match size: $25,000, $50,000, $100,000, or unstated - Suggested ask amount: equal to, 1.25×, or 1.5× the highest previous contribution\nThe purpose of this experiment was to assess whether the size or presence of a match increases the likelihood or amount of giving. Randomization ensures that any observed differences in outcomes can be causally attributed to the treatment."
  },
  {
    "objectID": "hw1_questions.html#data-overview",
    "href": "hw1_questions.html#data-overview",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data Overview",
    "text": "Data Overview\nWe begin by loading and exploring the structure of the dataset. The dataset includes 50,083 observations and 51 variables, covering treatment assignments, donor demographics, giving behavior, and geographic information."
  },
  {
    "objectID": "hw1_questions.html#data",
    "href": "hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nimport pandas as pd\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\nDescription\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n# Basic treatment/control breakdown\nprint(\"Unique match ratios:\", df[\"ratio\"].unique())\nprint(\"Treatment group size:\", df[df[\"treatment\"] == 1].shape[0])\nprint(\"Control group size:\", df[df[\"treatment\"] == 0].shape[0])\n\nUnique match ratios: ['Control', 1, 2, 3]\nCategories (4, object): ['Control' &lt; 1 &lt; 2 &lt; 3]\nTreatment group size: 33396\nControl group size: 16687\n\n\n\n# Summary statistics: mean donation rate and average donation by group\nsummary = df.groupby(\"treatment\")[[\"gave\", \"amount\"]].agg(['mean', 'count']).round(4)\nsummary\n\n\n\n\n\n\n\n\ngave\namount\n\n\n\nmean\ncount\nmean\ncount\n\n\ntreatment\n\n\n\n\n\n\n\n\n0\n0.0179\n16687\n0.8133\n16687\n\n\n1\n0.0220\n33396\n0.9669\n33396\n\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another."
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\ndef balance_test(var):\n    subset = df[['treatment', var]].dropna()\n    \n    # T-test\n    treated = subset[subset['treatment'] == 1][var]\n    control = subset[subset['treatment'] == 0][var]\n    t_stat, p_val = ttest_ind(treated, control, equal_var=False)\n    print(f\"\\nT-test for {var}: t = {t_stat:.3f}, p = {p_val:.4f}\")\n    \n    # Linear regression\n    model = smf.ols(f'{var} ~ treatment', data=subset).fit()\n    print(model.summary().tables[1])  # Coefficient table\n\n# Run for selected baseline variables\nfor var in ['mrm2', 'years', 'freq', 'female']:\n    balance_test(var)\n\n\nT-test for mrm2: t = 0.120, p = 0.9049\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\n\nT-test for years: t = -1.091, p = 0.2753\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      6.1359      0.043    144.023      0.000       6.052       6.219\ntreatment     -0.0575      0.052     -1.103      0.270      -0.160       0.045\n==============================================================================\n\nT-test for freq: t = -0.111, p = 0.9117\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      8.0473      0.088     91.231      0.000       7.874       8.220\ntreatment     -0.0120      0.108     -0.111      0.912      -0.224       0.200\n==============================================================================\n\nT-test for female: t = -1.754, p = 0.0795\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.2827      0.004     80.688      0.000       0.276       0.290\ntreatment     -0.0075      0.004     -1.758      0.079      -0.016       0.001\n==============================================================================\n\n\nTo evaluate whether the randomization process produced balanced groups, we compared key pre-treatment characteristics between the treatment and control groups using both t-tests and bivariate linear regressions. The variables examined included:\n• mrm2 (months since last donation)\n• years (since first donation)\n• freq (number of past donations)\n• female (gender)\nIn all cases, the p-values from both methods exceeded 0.05, indicating that none of the differences were statistically significant. This means the treatment and control groups were well-balanced across these characteristics, just as Table 1 in Karlan & List (2007) demonstrates.\nThis validation is crucial for the study’s internal validity, it confirms that differences in outcomes can be attributed to the treatment, not to pre-existing differences in donor behavior or demographics.\nfrom scipy.stats import ttest_ind import statsmodels.api as sm import statsmodels.formula.api as smf\n\n# Drop NA for mrm2\nbalance_df = df[['treatment', 'mrm2']].dropna()\n\n# T-TEST \ntreated = balance_df[balance_df['treatment'] == 1]['mrm2']\ncontrol = balance_df[balance_df['treatment'] == 0]['mrm2']\nt_stat, p_val = ttest_ind(treated, control, equal_var=False)\nprint(f\"T-test for mrm2: t = {t_stat:.3f}, p = {p_val:.4f}\")\n# LINEAR REGRESSION \nmodel = smf.ols('mrm2 ~ treatment', data=balance_df).fit()\nprint(model.summary().tables[1])  # Show only coefficient table\n\nT-test for mrm2: t = 0.120, p = 0.9049\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\n\n\nTo assess whether the treatment and control groups were statistically different before the intervention, we performed a balance test on the variable mrm2, which measures the number of months since the last donation.\nA t-test comparing the mean mrm2 between groups yielded a t-statistic of 0.120 and a p-value of 0.9049, indicating no statistically significant difference between the groups.\nWe also ran a linear regression of mrm2 on the treatment indicator. The estimated coefficient on treatment was 0.0137, with a p-value of 0.905 and a 95% confidence interval ranging from −0.211 to 0.238. These results confirm the same conclusion: the treatment group and control group had nearly identical values for this variable prior to the intervention.\nThis finding supports the integrity of the random assignment in the experiment. Since there are no significant differences in months since last donation, we can be confident that any observed differences in donation behavior after treatment are not due to baseline imbalances. This type of balance check is crucial for ensuring the internal validity of experimental results and mirrors the intent of Table 1 in Karlan & List (2007).\n\ndef run_balance_test(var):\n    subset = df[['treatment', var]].dropna()\n    \n    # T-test\n    treated = subset[subset['treatment'] == 1][var]\n    control = subset[subset['treatment'] == 0][var]\n    t_stat, p_val = ttest_ind(treated, control, equal_var=False)\n    print(f\"\\nT-test for {var}: t = {t_stat:.3f}, p = {p_val:.4f}\")\n    \n    # Regression\n    model = smf.ols(f'{var} ~ treatment', data=subset).fit()\n    print(model.summary().tables[1])  # Coefficient summary only\n\n# Run tests for mrm2, years, freq, and female\nfor variable in ['mrm2', 'years', 'freq', 'female']:\n    run_balance_test(variable)\n\n\nT-test for mrm2: t = 0.120, p = 0.9049\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\n\nT-test for years: t = -1.091, p = 0.2753\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      6.1359      0.043    144.023      0.000       6.052       6.219\ntreatment     -0.0575      0.052     -1.103      0.270      -0.160       0.045\n==============================================================================\n\nT-test for freq: t = -0.111, p = 0.9117\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      8.0473      0.088     91.231      0.000       7.874       8.220\ntreatment     -0.0120      0.108     -0.111      0.912      -0.224       0.200\n==============================================================================\n\nT-test for female: t = -1.754, p = 0.0795\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.2827      0.004     80.688      0.000       0.276       0.290\ntreatment     -0.0075      0.004     -1.758      0.079      -0.016       0.001\n==============================================================================\n\n\nTo verify the success of the randomization procedure, we conducted balance tests on four key pre-treatment variables:\n• mrm2 (months since last donation)\n• years (years since first donation)\n• freq (number of prior donations)\n• female (gender indicator)\nFor each variable, we conducted both a t-test and a bivariate linear regression of the variable on the treatment indicator. The results from both methods were consistent and are summarized below:\n• mrm2: The t-test returned a p-value of 0.905, and the regression coefficient on treatment was 0.0137. There is no evidence of a difference between groups on months since last donation.\n• years: The p-value was 0.275 in the t-test and 0.270 in the regression, with a treatment coefficient of −0.0575. Again, no significant difference.\n• freq: The p-value was 0.912 and the treatment coefficient was −0.0120, indicating complete balance in the number of prior donations.\n• female: This variable had a slightly lower p-value of 0.079 and a treatment coefficient of −0.0075. While not statistically significant at the 5% level, it is closer to the threshold, suggesting a small imbalance that is worth noting, though likely not practically important.\nOverall, these tests show that treatment and control groups were well-balanced across observable characteristics prior to the intervention. These results validate the success of the randomization procedure, ensuring that post-treatment differences in outcomes can be interpreted as causal effects of the matching donation treatment. This approach follows the same logic as Table 1 in Karlan & List (2007), which is used to demonstrate the internal validity of their experimental design.\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation."
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n# Sample real data without replacement\ndf_shuffled = df[['treatment', 'amount']].sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Encode treatment as +1 or -1 to compute difference\ndf_shuffled['diff'] = df_shuffled['amount'] * df_shuffled['treatment'] - df_shuffled['amount'] * (1 - df_shuffled['treatment'])\n\n# Cumulative average difference (treatment - control contribution)\ncumulative_avg = np.cumsum(df_shuffled['diff']) / np.arange(1, len(df_shuffled) + 1)\n\n# True difference from earlier\ntrue_diff = df[df['treatment'] == 1]['amount'].mean() - df[df['treatment'] == 0]['amount'].mean()\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Avg (Shuffled Real Data)')\nplt.axhline(y=true_diff, color='red', linestyle='--', label=f'True Difference ≈ {true_diff:.4f}')\nplt.title(\"Cumulative Average Difference\")\nplt.xlabel(\"Observation Index\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis plot displays the cumulative average difference in donation amounts (treatment minus control) based on the real dataset, where observations were randomly shuffled. Each point on the line represents the average difference after including one more data point.\nAt the beginning, the average fluctuates due to the small number of observations. As more data are added, the line smooths out and stabilizes near the true treatment effect (shown by the red dashed line).\nThis visually demonstrates the Law of Large Numbers: as the number of observations increases, the sample average converges to the true population difference, providing strong support that the observed treatment effect is consistent and not due to chance.\n\n\nCentral Limit Theorem\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Use observed donation behavior (binary) from your dataset\np_control = 0.018  # Control group's donation probability\np_treatment = 0.022  # Treatment group's donation probability\n\n# Sample sizes to simulate\nsample_sizes = [50, 200, 500, 1000]\nsimulations = 1000  # Number of experiments per sample size\n\n# Set up plot grid\nfig, axs = plt.subplots(2, 2, figsize=(14, 10))\naxs = axs.flatten()\n\n# Run simulations and plot\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n    for _ in range(simulations):\n        control_sample = np.random.binomial(1, p_control, size=n)\n        treatment_sample = np.random.binomial(1, p_treatment, size=n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        diffs.append(diff)\n\n    mean_diff = np.mean(diffs)\n    \n    # Plot histogram\n    axs[i].hist(diffs, bins=30, color='lightblue', edgecolor='black')\n    axs[i].axvline(x=0, color='red', linestyle='--', label='Zero')\n    axs[i].axvline(x=mean_diff, color='green', linestyle='-', label='Mean')\n    axs[i].set_title(f\"Sample Size = {n}\")\n    axs[i].set_xlabel(\"Average Difference (Treatment − Control)\")\n    axs[i].set_ylabel(\"Frequency\")\n    axs[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe four histograms illustrate the sampling distributions of the average difference in donation rates between the treatment and control groups, calculated from 1,000 simulated experiments at sample sizes of 50, 200, 500, and 1,000.\nIn each simulation, we randomly sampled from Bernoulli distributions (with p = 0.022 for treatment and p = 0.018 for control) and computed the average difference in donation rates. The histograms show the distribution of those average differences across simulations.\nSample Size = 50: The distribution is wide and irregular, showing high variability. The mean (green line) and the null hypothesis value (zero, red dashed line) are close together, and the effect is difficult to distinguish from noise.\nSample Size = 200: The distribution is smoother and narrower. The mean shifts slightly to the right, showing a small positive treatment effect. Zero is still near the center, but the result starts to show more separation.\nSample Size = 500: The distribution becomes clearly bell-shaped and narrower. The mean moves further away from zero, and now zero lies in the tail of the distribution, suggesting the treatment effect is more detectable.\nSample Size = 1000: The distribution is even tighter, with the average difference centered around the true effect. The null value (zero) is well into the tail, meaning that if this were real experimental data, the treatment effect would likely be statistically significant.\nThese plots demonstrate both the Central Limit Theorem (distributions become normal as n increases) and the Law of Large Numbers (sample averages converge to true values). They also show how larger sample sizes increase the reliability of statistical estimates and reduce the likelihood of failing to detect real effects."
  }
]