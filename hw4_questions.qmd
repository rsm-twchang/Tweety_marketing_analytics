---
title: "Key Drivers Analysis"
format: html
author: "Tweety"
date: "2025-06-06"
callout-appearance: minimal
---

### Introduction

This analysis applies the **K-Means clustering algorithm** to the *Palmer Penguins* dataset, using two key morphological measurements: **bill length** and **flipper length**. The goal is to explore whether natural groupings of penguins can be uncovered without using species labels, and how well these groupings align with biological categories.

Unlike supervised learning, clustering is an **unsupervised machine learning method**, meaning it identifies structure in the data without relying on labeled outcomes. K-Means is one of the most common techniques for this purpose, partitioning observations into `k` clusters based on feature similarity.

In this project, we:
- Implement K-Means both **from scratch** and using **scikit-learn**,
- Visualize the clustering process across iterations,
- Compare our custom implementation to the built-in version,
- Evaluate different values of `k` using **WCSS** and **silhouette scores** to identify the optimal number of clusters.

This exercise provides both technical practice in unsupervised learning and biological insight into how penguin species differ morphologically.

### Dataset Description

The *Palmer Penguins* dataset provides biological measurements for three penguin species native to islands in the Palmer Archipelago, Antarctica: **Adélie**, **Chinstrap**, and **Gentoo**.

Each row in the dataset corresponds to a single penguin and includes variables such as:

- `species`: species name (Adélie, Chinstrap, or Gentoo)
- `island`: island where the penguin was observed
- `bill_length_mm`: bill length (mm)
- `bill_depth_mm`: bill depth (mm)
- `flipper_length_mm`: flipper length (mm)
- `body_mass_g`: body mass (g)
- `sex`: biological sex (if known)
- `year`: year of observation

For this analysis, we focus on two numerical features:
- **Bill Length (`bill_length_mm`)**
- **Flipper Length (`flipper_length_mm`)**

These measurements are used as inputs to clustering algorithms to explore natural groupings in the data without relying on species labels.

### Data
```{python}
#| warning: false
#| echo: false
import warnings
warnings.filterwarnings("ignore")
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
```

```{python}
#| echo: false
penguins = pd.read_csv("/Users/qqtweety/Downloads/mgta495/palmer_penguins.csv")
penguins.head()
```

```{python}
#| echo: false
penguins_clean = penguins[['bill_length_mm', 'flipper_length_mm']].dropna().reset_index(drop=True)
X = penguins_clean.values
```

```{python}
#| echo: true
def plot_clusters(X, centroids, labels, title):
    plt.figure(figsize=(8, 6))
    for i in range(np.max(labels) + 1):
        plt.scatter(X[labels == i, 0], X[labels == i, 1], label=f'Cluster {i+1}')
    plt.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x', s=100, label='Centroids')
    plt.xlabel('Bill Length (mm)')
    plt.ylabel('Flipper Length (mm)')
    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.show()
```

```{python}
#| echo: true
# Custom K-Means Implementation
def kmeans_custom(X, k=3, max_iters=10, plot_each_step=True):
    np.random.seed(42)
    initial_idx = np.random.choice(X.shape[0], k, replace=False)
    centroids = X[initial_idx]

    for iteration in range(max_iters):
        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)
        labels = np.argmin(distances, axis=1)

        if plot_each_step:
            plot_clusters(X, centroids, labels, f'Custom KMeans - Iteration {iteration + 1}')

        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])
        if np.allclose(centroids, new_centroids):
            break
        centroids = new_centroids

    return centroids, labels
```
```{python}
#| echo: true
# Run the custom KMeans and visualize
custom_centroids, custom_labels = kmeans_custom(X, k=3)
```
```{python}
#| echo: true
# Run the built-in KMeans for comparison
sk_model = KMeans(n_clusters=3, random_state=42)
sk_labels = sk_model.fit_predict(X)
sk_centroids = sk_model.cluster_centers_

# Plot result from sklearn
plot_clusters(X, sk_centroids, sk_labels, "Built-in KMeans Final Result")
```
```{python}
#| echo: true
# Show centroid comparison
print("Custom Centroids:\n", custom_centroids)
print("\nBuilt-in Centroids:\n", sk_centroids)
```

## Interpretation of Results

The custom and built-in KMeans algorithms produced comparable clustering outcomes. The centroids differed by only 1–5 mm per dimension, confirming convergence to similar solutions.

The built-in method placed some centroids slightly farther along the flipper-length axis, likely due to finer convergence or sensitivity to outliers.

Cluster interpretation suggests:
- Cluster 1 groups penguins with shorter bills and flippers, likely Adélie.
- Cluster 2 includes those with the longest bills and flippers, likely Gentoo.
- Cluster 3 covers the intermediate range, possibly Chinstrap.

Despite random initialization, the small Euclidean distances between corresponding centroids reflect stable clustering across both implementations.

```{python}
#| echo: true
from sklearn.metrics import silhouette_score
# Store metrics for different K values
ks = range(2, 8)
wcss = []  # within-cluster sum of squares (inertia)
silhouette_scores = []

for k in ks:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X)
    wcss.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X, labels))

# Plot WCSS and Silhouette Score
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
ax1.plot(ks, wcss, marker='o')
ax1.set_title('Within-Cluster Sum of Squares (WCSS)')
ax1.set_xlabel('Number of Clusters (k)')
ax1.set_ylabel('WCSS')
ax2.plot(ks, silhouette_scores, marker='o')
ax2.set_title('Silhouette Score')
ax2.set_xlabel('Number of Clusters (k)')
ax2.set_ylabel('Silhouette Score')
plt.tight_layout()
plt.show()
```

```{python}
#| echo: true
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Fit KMeans for K=3
kmeans_3 = KMeans(n_clusters=3, random_state=42)
labels_3 = kmeans_3.fit_predict(X)
centroids_3 = kmeans_3.cluster_centers_

# Plotting function
def plot_kmeans_clusters(X, labels, centroids):
    plt.figure(figsize=(8, 6))
    colors = ['red', 'gold', 'magenta']
    for i in range(3):
        plt.scatter(X[labels == i, 0], X[labels == i, 1], s=10, color=colors[i], alpha=0.7)
        plt.scatter(centroids[i, 0], centroids[i, 1], c='black', s=100, edgecolors='black', linewidths=1.5)
    plt.title('K-Means Clustering (K = 3)')
    plt.xlabel('Bill Length (mm)')
    plt.ylabel('Flipper Length (mm)')
    plt.grid(True)
    plt.tight_layout()
    plt.show()
plot_kmeans_clusters(X, labels_3, centroids_3)
```

```{python}
#| echo: true
# Return the raw values for interpretation
list(zip(ks, wcss, silhouette_scores))
```

### Evaluation of Cluster Quantity

To determine the optimal number of clusters, we computed the **within-cluster sum of squares (WCSS)** and **silhouette scores** for K ranging from 2 to 7.

### Metric Summary

| K | WCSS     | Silhouette Score |
|---|----------|------------------|
| 2 | 20949.79 | **0.612**        |
| 3 | 14269.56 | 0.456            |
| 4 | 9587.14  | 0.445            |
| 5 | 7597.61  | 0.410            |
| 6 | 6326.31  | 0.414            |
| 7 | 6030.08  | 0.370            |

**Interpretation**

- **WCSS** decreases with higher K, as expected, showing improved compactness.
- **Silhouette score** peaks at **K=2**, indicating the clearest separation between clusters.
- From **K=3 onward**, silhouette values steadily decline, suggesting diminishing cluster quality.

**Recommended K**

**K = 2** offers the best trade-off between compactness and separation. It is the most natural grouping for this dataset based on these metrics.
---

```{python}
#| echo: true
# Set seed and parameters
np.random.seed(42)
n = 100
# Generate features
x1 = np.random.uniform(-3, 3, n)
x2 = np.random.uniform(-3, 3, n)
# Define boundary and outcome
boundary = np.sin(4 * x1) + x1
y = (x2 > boundary).astype(int)
# Create DataFrame
dat = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})
dat.head()
```
## Synthetic Dataset for K-Nearest Neighbors

We generated a synthetic dataset with two features, `x1` and `x2`, uniformly drawn from [-3, 3]. The binary outcome variable `y` is assigned based on a non-linear boundary:

$$
y = 
\begin{cases}
1 & \text{if } x_2 > \sin(4x_1) + x_1 \\
0 & \text{otherwise}
\end{cases}
$$

This creates a wiggly decision boundary ideal for testing non-linear classifiers like KNN.

**Example Records**

| x1       | x2       | y |
|----------|----------|---|
| -0.75    | -2.81    | 0 |
|  2.70    |  0.82    | 0 |
|  1.39    | -1.11    | 0 |
|  0.59    |  0.05    | 0 |
| -2.06    |  2.45    | 1 |

### Visualization
```{python}
#| echo: true
# Plot the synthetic data with decision boundary
plt.figure(figsize=(8, 6))
colors = ['red' if label == 0 else 'blue' for label in dat['y']]
plt.scatter(dat['x1'], dat['x2'], c=colors, alpha=0.7, edgecolor='k', label='Data points')
x1_sorted = np.linspace(-3, 3, 300)
boundary_curve = np.sin(4 * x1_sorted) + x1_sorted
plt.plot(x1_sorted, boundary_curve, color='black', linestyle='--', label='True decision boundary')

plt.xlabel("x1")
plt.ylabel("x2")
plt.title("Synthetic Data and True Decision Boundary")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
```
---

```{python}
#| echo: true
# Generate a test dataset using a different random seed
np.random.seed(24)
n_test = 100

# Generate test features
x1_test = np.random.uniform(-3, 3, n_test)
x2_test = np.random.uniform(-3, 3, n_test)

# Apply the same decision boundary rule
boundary_test = np.sin(4 * x1_test) + x1_test
y_test = pd.Series((x2_test > boundary_test).astype(int), dtype='category')

# Create the test DataFrame
test_dat = pd.DataFrame({'x1': x1_test, 'x2': x2_test, 'y': y_test})
test_dat.head()
```

## Test Dataset: Synthetic Data for KNN Evaluation

A test dataset of 100 points was generated using uniform random values for `x1` and `x2`, drawn from the range [-3, 3]. The binary label `y` was assigned based on a non-linear boundary:

$$
y = 
\begin{cases}
1 & \text{if } x_2 > \sin(4x_1) + x_1 \\
0 & \text{otherwise}
\end{cases}
$$

This boundary creates a wavy separation ideal for testing flexible classifiers like K-Nearest Neighbors.

### Sample Data

| x1       | x2       | y |
|----------|----------|---|
| 2.76     | 1.61     | 0 |
| 1.20     | -0.15    | 0 |
| 2.99     | 0.11     | 0 |
| -1.68    | 2.67     | 1 |
| -0.83    | 0.86     | 1 |

**Interpretation of the Plot**
```{python}
#| echo: true
plt.figure(figsize=(8, 6))

# Color-coded test points
for class_value, color, label in zip([0, 1], ['red', 'blue'], ['y = 0', 'y = 1']):
    subset = test_dat[test_dat['y'] == class_value]
    plt.scatter(subset['x1'], subset['x2'], c=color, label=label, edgecolor='k', s=50)

# Plot the true decision boundary
x1_vals = np.linspace(-3, 3, 300)
boundary_curve = np.sin(4 * x1_vals) + x1_vals
plt.plot(x1_vals, boundary_curve, 'k--', label='True boundary')

plt.xlabel("x1")
plt.ylabel("x2")
plt.title("Test Dataset with True Decision Boundary")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
```

```{python}
#| echo: true
from collections import Counter

# Step-by-step manual KNN implementation (k=5)
def knn_predict(X_train, y_train, X_test, k=5):
    predictions = []
    for x in X_test:
        # Compute Euclidean distances
        distances = np.linalg.norm(X_train - x, axis=1)
        # Get indices of k nearest neighbors
        nn_indices = distances.argsort()[:k]
        # Get the most common class among nearest neighbors
        nn_labels = y_train[nn_indices]
        most_common = Counter(nn_labels).most_common(1)[0][0]
        predictions.append(most_common)
    return np.array(predictions)

# Prepare data
X_train = dat[['x1', 'x2']].values
y_train = dat['y'].astype(int).values
X_test = test_dat[['x1', 'x2']].values
y_test = test_dat['y'].astype(int).values

# Manual prediction
manual_preds = knn_predict(X_train, y_train, X_test, k=5)

# Check accuracy of manual KNN
manual_accuracy = np.mean(manual_preds == y_test)

# Compare to sklearn's KNN
from sklearn.neighbors import KNeighborsClassifier
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)
sklearn_preds = knn_model.predict(X_test)
sklearn_accuracy = np.mean(sklearn_preds == y_test)

manual_accuracy, sklearn_accuracy
```

## Manual KNN Implementation and Validation

We implemented the K-Nearest Neighbors algorithm from scratch using Euclidean distance and a majority vote among the `k = 5` nearest neighbors.

To verify the implementation, we compared it with `KNeighborsClassifier` from `scikit-learn`.

### Accuracy Comparison

| Method        | Accuracy |
|---------------|----------|
| Manual KNN    | 92%      |
| Sklearn KNN   | 92%      |

Both methods produced identical results, confirming the correctness of the custom implementation. This validates our understanding of the KNN algorithm and the reproducibility of results using both custom and built-in tools.
---

## Tuning K in K-Nearest Neighbors

```{python}
#| echo: true
# Manual KNN function
def knn_predict(X_train, y_train, X_test, k=5):
    predictions = []
    for x in X_test:
        distances = np.linalg.norm(X_train - x, axis=1)
        nn_indices = distances.argsort()[:k]
        nn_labels = y_train[nn_indices]
        most_common = Counter(nn_labels).most_common(1)[0][0]
        predictions.append(most_common)
    return np.array(predictions)

# Define training and test sets
X_train = dat[['x1', 'x2']].values
y_train = dat['y'].astype(int).values
X_test = test_dat[['x1', 'x2']].values
y_test = test_dat['y'].astype(int).values

# Compute accuracy for k = 1 to 30
k_values = range(1, 31)
accuracies = [np.mean(knn_predict(X_train, y_train, X_test, k) == y_test) for k in k_values]
```

We evaluated classification accuracy on the test dataset for `k = 1` to `k = 30` using our custom KNN function.

```{python}
#| fig-cap: "Test accuracy of manual KNN for k = 1 to 30."
plt.plot(k_values, accuracies, marker='o')
plt.title("KNN Accuracy on Test Data for k = 1 to 30")
plt.xlabel("Number of Neighbors (k)")
plt.ylabel("Accuracy")
plt.grid(True)
plt.tight_layout()
plt.show()
```


## Dataset Description

The `yogurt_data` dataset contains information on consumer yogurt purchases. Each row represents an individual shopping trip, including the quantity of yogurt purchased and prices for five different yogurt brands.

Key variables include:

- `id`: unique identifier for each customer
- `choice`: the brand chosen during the shopping trip
- `price1` to `price5`: prices of the five competing yogurt brands
- `quantity`: number of yogurt units purchased

This dataset is commonly used for analyzing consumer choice behavior, price sensitivity, and brand preference using models like multinomial logit or KNN classification.

```{python}
yogurt = pd.read_csv("/Users/qqtweety/Downloads/mgta495/yogurt_data.csv")
yogurt.head()
```
```{python}
# Reshape dataset to long format suitable for LC-MNL
import numpy as np

# Create long-format structure
long_df = pd.DataFrame()

# Loop through 4 alternatives per observation
for i in range(1, 5):
    temp = pd.DataFrame({
        'id': yogurt['id'],
        'alt': i,
        'choice': yogurt[f'y{i}'],
        'feature': yogurt[f'f{i}']
    })
    long_df = pd.concat([long_df, temp], ignore_index=True)

# Sort by id and alt for clarity
long_df = long_df.sort_values(by=['id', 'alt']).reset_index(drop=True)
long_df.head(8)
```

```{python}
#| echo: true
from scipy.special import logsumexp
from scipy.optimize import minimize
# Prepare data
df = long_df.copy()
n_alts = df['alt'].nunique()
n_obs = df['id'].nunique()

# Pivot to get feature matrix per choice set
X = df.pivot(index='id', columns='alt', values='feature').values  # shape: (n_obs, 4)
Y = df.pivot(index='id', columns='alt', values='choice').values   # shape: (n_obs, 4)

# Log-likelihood function for latent-class MNL with 2 segments
def latent_class_loglik(params):
    beta1, beta2, lamb = params[:1], params[1:2], params[2]
    pi1 = np.exp(lamb) / (1 + np.exp(lamb))
    pi2 = 1 - pi1

    # Utilities for each segment
    V1 = X * beta1
    V2 = X * beta2

    # Choice probabilities
    P1 = np.exp(V1 - logsumexp(V1, axis=1, keepdims=True))
    P2 = np.exp(V2 - logsumexp(V2, axis=1, keepdims=True))

    # Segment-level likelihoods
    LL1 = np.sum(Y * np.log(P1 + 1e-12), axis=1)
    LL2 = np.sum(Y * np.log(P2 + 1e-12), axis=1)

    # Full log-likelihood (marginalizing over segments)
    LL = np.log(pi1 * np.exp(LL1) + pi2 * np.exp(LL2) + 1e-12)
    return -np.sum(LL)

# Initial parameter guess: [β1, β2, λ]
init_params = np.array([0.1, 0.1, 0.0])
result = minimize(latent_class_loglik, init_params, method='BFGS')

# Output
estimated_params = result.x
beta1_hat, beta2_hat, lambda_hat = estimated_params[0], estimated_params[1], estimated_params[2]
pi1_hat = np.exp(lambda_hat) / (1 + np.exp(lambda_hat))
pi2_hat = 1 - pi1_hat

beta1_hat, beta2_hat, pi1_hat, pi2_hat
```

## Latent-Class MNL Estimation (Kamakura & Russell, 1989)

We estimate a two-segment latent-class MNL model using the Yogurt dataset. Each segment has its own utility sensitivity parameter (β), and segment membership is probabilistic via a logistic transformation of λ:

\[
\pi_1 = \frac{e^{\lambda}}{1 + e^{\lambda}}, \quad \pi_2 = 1 - \pi_1
\]

The unconditional choice probability is a weighted sum over segment-specific MNL probabilities:

\[
P_i(j) = \sum_{s=1}^{2} \pi_s \cdot P_i(j \mid s)
\]

---

## Dataset Structure

Each observation includes four alternatives per choice set. Data is reshaped into long format with:

- `id`: choice occasion  
- `alt`: alternative (1–4)  
- `feature`: product attribute  
- `choice`: binary indicator of selection

## Estimation Results
```{python}
print(f"β₁ = {beta1_hat:.3f}")
print(f"β₂ = {beta2_hat:.3f}")
print(f"π₁ = {pi1_hat:.3f}")
print(f"π₂ = {pi2_hat:.3f}")
```
---

```{python}
#| echo: true
# Create long-format structure
long_data = pd.DataFrame()

for i in range(1, 5):  # 4 products
    temp = pd.DataFrame({
        'id': yogurt['id'],
        'alt': i,
        'choice': yogurt[f'y{i}'],
        'feature': yogurt[f'f{i}'],
        'price': yogurt[f'p{i}']
    })
    long_data = pd.concat([long_data, temp], ignore_index=True)

long_data = long_data.sort_values(['id', 'alt']).reset_index(drop=True)
long_data.head(8)
```
```{python}
#| echo: true
# Prepare matrices
X_price = long_data.pivot(index='id', columns='alt', values='price').values
X_feature = long_data.pivot(index='id', columns='alt', values='feature').values
Y = long_data.pivot(index='id', columns='alt', values='choice').values

# Log-likelihood function for LC-MNL with 2 variables per segment
def log_likelihood(params):
    beta1 = params[0:2]   # [price, feature] for segment 1
    beta2 = params[2:4]   # [price, feature] for segment 2
    lamb = params[4]

    pi1 = np.exp(lamb) / (1 + np.exp(lamb))
    pi2 = 1 - pi1

    # Utilities for each segment
    V1 = beta1[0] * X_price + beta1[1] * X_feature
    V2 = beta2[0] * X_price + beta2[1] * X_feature

    # Choice probabilities
    P1 = np.exp(V1 - logsumexp(V1, axis=1, keepdims=True))
    P2 = np.exp(V2 - logsumexp(V2, axis=1, keepdims=True))

    # Segment-level likelihood
    LL1 = np.sum(Y * np.log(P1 + 1e-12), axis=1)
    LL2 = np.sum(Y * np.log(P2 + 1e-12), axis=1)

    LL_total = np.log(pi1 * np.exp(LL1) + pi2 * np.exp(LL2) + 1e-12)
    return -np.sum(LL_total)

# Initial guess: [β1_price, β1_feature, β2_price, β2_feature, lambda]
init = np.array([-10, 1, -10, 1, 0])  # negative price sensitivity, positive feature

# Estimate
result = minimize(log_likelihood, init, method='BFGS')
beta1_est = result.x[0:2]
beta2_est = result.x[2:4]
lambda_est = result.x[4]
pi1_est = np.exp(lambda_est) / (1 + np.exp(lambda_est))
pi2_est = 1 - pi1_est

beta1_est, beta2_est, pi1_est, pi2_est
```

## Latent-Class Multinomial Logit (LC-MNL) Estimation

The yogurt dataset includes anonymized consumer IDs (`id`), binary indicators of purchased products (`y1`–`y4`), whether products were advertised in-store (`f1`–`f4`), and price-per-ounce values (`p1`–`p4`).

Each row represents a consumer's single purchase occasion across four yogurt options. For example, consumer 1 selected yogurt 4 at a price of 0.079/oz, with no items advertised. Consumers 2–7 all chose yogurt 2.

To estimate a latent-class multinomial logit model, the dataset was reshaped from **wide** to **long** format, creating one row per consumer–product pair. Key variables used were:
- `choice`: 1 if the product was selected, 0 otherwise
- `price`: per-ounce price
- `feature`: whether the item was advertised

We estimated a two-segment LC-MNL model using price and feature as predictors of utility.

**Segment-Level Estimates**

| Segment | β<sub>price</sub> | β<sub>feature</sub> | π (Share) |
|:--------|------------------:|--------------------:|----------:|
| 1       | 12.91             | 14.64               | 42.7%     |
| 2       | 9.18              | −12.17              | 57.3%     |

**Interpretation**

- **Segment 1** is highly responsive to both lower prices and in-store advertising.
- **Segment 2** exhibits lower price sensitivity and reacts negatively to advertising.
- Segment proportions (42.7% vs. 57.3%) indicate heterogeneous preference patterns among consumers.

These results suggest the presence of two distinct market segments: one promotion-sensitive, the other resistant to marketing signals.

---
```{python}
long_data = pd.DataFrame({
    "id": [1, 1, 1, 1, 2, 2, 2, 2],
    "alt": [1, 2, 3, 4, 1, 2, 3, 4],
    "choice": [0, 0, 0, 1, 0, 1, 0, 0],
    "feature": [0, 0, 0, 0, 0, 0, 0, 0],
    "price": [0.108, 0.081, 0.061, 0.079, 0.108, 0.098, 0.064, 0.075]
})

# Add intercept (required for statsmodels MNLogit)
long_data['intercept'] = 1

# Define predictors and response
X = long_data[['intercept', 'price']]
y = long_data['choice']

# Fit the Multinomial Logit Model
model = sm.MNLogit(y, X)
mnl_result = model.fit(disp=False)

# Extract and show coefficient summary
mnl_summary = mnl_result.summary()
mnl_summary.tables[1]  # Coefficients table only
```