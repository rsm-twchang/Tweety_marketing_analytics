---
title: "Key Drivers Analysis"
author: "Tweety"
date: today
---

This post implements a few measure of variable importance, interpreted as a key drivers analysis, for certain aspects of a payment card on customer satisfaction with that payment card.

```{python}
#| warning: false
#| echo: false
import warnings
warnings.filterwarnings("ignore")
import pandas as pd
```

```{python}
#| echo: false
data_drivers = pd.read_csv("/Users/qqtweety/Downloads/mgta495/data_for_drivers_analysis.csv")
data_drivers.head()
```
```{python}
#| echo: false
penguins = pd.read_csv("/Users/qqtweety/Downloads/mgta495/palmer_penguins.csv")
penguins.head()
```
```{python}
#| echo: false
yogurt = pd.read_csv("/Users/qqtweety/Downloads/mgta495/yogurt_data.csv")
yogurt.head()
```
```{python}
#| echo: false
# Define the target and driver variables
target_var = 'satisfaction'
non_drivers = ['brand', 'id', target_var]
driver_vars = [col for col in data_drivers.columns if col not in non_drivers]

# Calculate Pearson correlations between each driver and satisfaction
correlations = data_drivers[driver_vars].corrwith(data_drivers[target_var]).sort_values(ascending=False)

print("Pearson Correlation Coefficients with 'satisfaction")
for var, coef in correlations.items():
    print(f"{var:<12} : {coef:.3f}")
```

## Key Driver Analysis: Pearson Correlation Summary

Below is a table summarizing the linear relationship between each driver and satisfaction, based on Pearson correlation coefficients.

| **Driver**   | **Pearson Corr** | **Interpretation**                                                                 |
|--------------|------------------|-------------------------------------------------------------------------------------|
| `trust`      | 0.256            | Strongest linear association. Users who find the brand trustworthy are more likely to report higher satisfaction. |
| `impact`     | 0.255            | Closely tied with trust — customers who feel the brand makes an impact tend to rate it higher. |
| `service`    | 0.251            | Good service is a clear satisfaction driver.                                       |
| `easy`       | 0.213            | Easier experiences promote better satisfaction.                                    |
| `appealing`  | 0.208            | Aesthetics and appeal contribute, but less than functionality.                     |
| `rewarding`  | 0.195            | Lower but still relevant; suggests gamification or value might help.               |
| `build`      | 0.192            | Slightly weaker — maybe users are less concerned about construction quality.       |
| `differs`    | 0.185            | Differentiation matters but modestly.                                              |
| `popular`    | 0.171            | Least influential of the group — social proof may not strongly sway satisfaction.  |
:::

```{python}
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

#| echo: false
# Define variables
target = 'satisfaction'
exclude = ['brand', 'id', target]
X = data_drivers.drop(columns=exclude)
y = data_drivers[target]

# Standardize X and y
scaler_X = StandardScaler()
X_scaled = scaler_X.fit_transform(X)

scaler_y = StandardScaler()
y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).ravel()

# Fit linear regression
model = LinearRegression()
model.fit(X_scaled, y_scaled)

# Create results table
standardized_coefs = pd.Series(model.coef_, index=X.columns).sort_values(ascending=False)

# Print formatted output
print("Standardized Regression Coefficients (Beta Weights)")
for var, coef in standardized_coefs.items():
    print(f"{var:<12} : {coef:.3f}")
```

## Key Driver Analysis: Standardized Regression Coefficients (Beta Weights)

Standardized coefficients represent the **unique impact** of each driver on `satisfaction`, after controlling for all other drivers. Higher values indicate a stronger, independent contribution to explaining satisfaction, even if the variables are correlated.

| **Driver**   | **Beta Weight** | **Interpretation** |
|--------------|------------------|---------------------|
| `impact`     | 0.128            | Strongest independent predictor. Customers who perceive the brand as impactful tend to rate their satisfaction higher, even when accounting for other factors. |
| `trust`      | 0.116            | Trust is also a significant driver — indicating it’s not just correlated, but uniquely important for satisfaction. |
| `service`    | 0.088            | Good service contributes meaningfully to satisfaction on its own. |
| `appealing`  | 0.034            | Visual or emotional appeal matters, but less than functional aspects. |
| `differs`    | 0.028            | Being different adds value, though it's a minor unique driver. |
| `easy`       | 0.022            | Ease of use plays a small but measurable role in satisfaction. |
| `build`      | 0.020            | Build quality is not a strong independent factor in this model. |
| `popular`    | 0.017            | Social proof has low standalone impact once other variables are considered. |
| `rewarding`  | 0.005            | Perceived rewards offer minimal unique explanatory power in this setting. |

> These weights are based on a standardized linear regression, where all variables were z-scored. Coefficients reflect the expected change in satisfaction (in standard deviations) for a one standard deviation increase in each driver.
:::

```{python}
#| echo: false
from sklearn.metrics import r2_score
X = data_drivers.drop(columns=['brand', 'id', 'satisfaction'])
y = data_drivers['satisfaction']

# Standardize
scaler_X = StandardScaler()
X_scaled = scaler_X.fit_transform(X)

scaler_y = StandardScaler()
y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).ravel()

# Full model R²
model_full = LinearRegression().fit(X_scaled, y_scaled)
r2_full = r2_score(y_scaled, model_full.predict(X_scaled))

# Compute usefulness
usefulness = {}
for col in X.columns:
    X_reduced = X.drop(columns=[col])
    X_reduced_scaled = StandardScaler().fit_transform(X_reduced)
    r2_reduced = r2_score(y_scaled, LinearRegression().fit(X_reduced_scaled, y_scaled).predict(X_reduced_scaled))
    usefulness[col] = r2_full - r2_reduced

usefulness_series = pd.Series(usefulness).sort_values(ascending=False)
print(" Usefulness (Drop in R² when removed):")
for var, delta_r2 in usefulness_series.items():
    print(f"{var:<12} : {delta_r2:.4f}")
```

## Key Driver Analysis: Usefulness (Drop in R² When Removed)

The **usefulness metric** quantifies the unique contribution of each driver to the overall explanatory power (R²) of the regression model. A higher drop in R² when a driver is removed indicates that the variable is more **indispensable** for predicting satisfaction.

| **Driver**   | **ΔR² (Usefulness)** | **Interpretation** |
|--------------|----------------------|---------------------|
| `impact`     | 0.0112               | Most essential driver. Removing it drops the model’s predictive power the most, suggesting a central role in driving satisfaction. |
| `trust`      | 0.0082               | Also critical. Trust uniquely enhances the model’s explanatory strength. |
| `service`    | 0.0047               | Makes a moderate contribution, reinforcing the importance of good service. |
| `appealing`  | 0.0007               | Plays a minor role — adds some value to the model but not decisively. |
| `differs`    | 0.0006               | Suggests a weak but measurable influence on satisfaction. |
| `easy`       | 0.0003               | Contributes very little when considered independently. |
| `build`      | 0.0003               | Like ease of use, has minimal stand-alone predictive power. |
| `popular`    | 0.0002               | Popularity is not a meaningful unique driver. |
| `rewarding`  | 0.0000               | No measurable unique contribution to satisfaction in this model. |

> These values are computed by measuring the drop in R² when each variable is removed from the standardized linear regression. It answers the question: “How much worse would the model perform if we left this out?”
:::

```{python}
#| echo: false
from sklearn.decomposition import PCA
X = data_drivers.drop(columns=['brand', 'id', 'satisfaction'])
y = data_drivers['satisfaction']

# Standardize X and y
X_scaled = StandardScaler().fit_transform(X)
y_scaled = StandardScaler().fit_transform(y.values.reshape(-1, 1)).ravel()

# Run PCA on X
pca = PCA()
Z = pca.fit_transform(X_scaled)

# Regress y on PCA-transformed data
reg = LinearRegression().fit(Z, y_scaled)
beta_pca = reg.coef_

# Calculate variance contributions
# Square the betas and map back to original features
phi = (pca.components_[:len(X.columns), :]**2).T @ (beta_pca[:len(X.columns)]**2)

# Normalize to get proportions
johnson_weights = pd.Series(phi, index=X.columns)
johnson_weights = johnson_weights / johnson_weights.sum()

johnson_weights = johnson_weights.sort_values(ascending=False)

print(" Johnson’s Relative Weights (Proportional Contribution to R²):")
for var, wt in johnson_weights.items():
    print(f"{var:<12} : {wt:.4f}")
```

## Key Driver Analysis: Johnson’s Relative Weights

**Johnson’s Relative Weights** estimate how much each driver contributes to the model’s total R² (explained variance), even in the presence of multicollinearity. This technique fairly distributes explanatory power across variables that may be highly correlated.

| **Driver**   | **Johnson Weight** | **Interpretation** |
|--------------|--------------------|---------------------|
| `build`      | 0.1738             | The most influential driver overall. Build quality plays a central role in shaping satisfaction, even when correlated with other variables. |
| `service`    | 0.1421             | A core contributor — service-related experiences are clearly impactful. |
| `trust`      | 0.1270             | Trustworthiness of the brand is a major underlying influence on satisfaction. |
| `rewarding`  | 0.1233             | Suggests that when customers feel rewarded, it meaningfully enhances satisfaction. |
| `impact`     | 0.1185             | Impact has slightly less unique variance than in previous metrics, but still ranks highly. |
| `appealing`  | 0.0979             | The emotional or aesthetic appeal contributes meaningfully, though less than functional traits. |
| `easy`       | 0.0909             | Ease of use plays a supporting but important role. |
| `popular`    | 0.0667             | Popularity has modest standalone influence in the presence of other drivers. |
| `differs`    | 0.0598             | Differentiation contributes the least, suggesting it overlaps with other more dominant drivers. |

> These weights represent each driver’s **share of the total R²** and collectively sum to 1. A weight of 0.17 means that variable explains 17% of the model’s total explanatory power.
:::

```{python}
#| echo: false
from sklearn.ensemble import RandomForestRegressor
X = data_drivers.drop(columns=['brand', 'id', 'satisfaction'])
y = data_drivers['satisfaction']

# Train Random Forest
rf = RandomForestRegressor(random_state=42, n_estimators=100)
rf.fit(X, y)

# Extract Gini Importance
gini_importance = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)

print(" Random Forest Gini Importances:")
for var, imp in gini_importance.items():
    print(f"{var:<12} : {imp:.4f}")
```

## Key Driver Analysis: Random Forest Gini Importance

Random Forest Gini Importance reflects how much each variable **reduces prediction error** (Gini impurity) across all decision trees in the ensemble. A higher value means the variable was more frequently and effectively used in the model’s internal decision splits.

| **Driver**   | **Gini Importance** | **Interpretation** |
|--------------|---------------------|---------------------|
| `trust`      | 0.1559              | Most powerful tree-based predictor. Trust is repeatedly selected at decision splits, highlighting its consistent impact on satisfaction. |
| `impact`     | 0.1408              | A highly useful variable for nonlinear or interaction effects — central in the forest’s logic. |
| `service`    | 0.1297              | Provides valuable splits in the model, supporting strong service as a satisfaction driver. |
| `build`      | 0.1023              | Contributes to model accuracy, particularly in interaction-heavy paths. |
| `rewarding`  | 0.1011              | Emerges as important in trees, more than in regression — may have nonlinear effects. |
| `easy`       | 0.0999              | A solid secondary feature, used frequently in lower splits. |
| `popular`    | 0.0949              | Surprisingly effective in forest models — perhaps captures interaction or contextual variance. |
| `differs`    | 0.0899              | Plays a supporting role in many decision trees. |
| `appealing`  | 0.0855              | Lowest of the group, yet still contributes to satisfaction prediction in complex ways. |

> 🔍 **Gini importances are relative** and **do not sum to 1**. They are model-specific measures of how frequently and effectively a feature is used to split nodes across the ensemble of trees.
:::

```{python}
#| echo: false
from sklearn.ensemble import RandomForestRegressor
from sklearn.inspection import permutation_importance
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
X = data_drivers.drop(columns=['brand', 'id', 'satisfaction'])
y = data_drivers['satisfaction']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.25)

# Train model
rf = RandomForestRegressor(random_state=42, n_estimators=100)
rf.fit(X_train, y_train)

# Evaluate baseline performance
baseline_r2 = r2_score(y_test, rf.predict(X_test))
print(f"Baseline R²: {baseline_r2:.4f}")

# Compute permutation importance
perm_result = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42)

# Format results
perm_importance = pd.Series(perm_result.importances_mean, index=X.columns).sort_values(ascending=False)

# Print results
print(" Permutation Importances (based on R² drop):")
for feature, score in perm_importance.items():
    print(f"{feature:<12} : {score:.4f}")
```

## Key Driver Analysis: Permutation Importance (Random Forest)

Permutation Importance quantifies how much the model’s performance drops when each driver is randomly shuffled. A larger drop in R² means the variable is more important to the model’s predictive ability.

> **Baseline R² on test set**: `0.0603`  
> The model explains ~6% of the variation in satisfaction on unseen data — a modest fit, possibly due to limited feature coverage or inherent randomness in satisfaction.

| **Driver**   | **ΔR² (Permutation)** | **Interpretation** |
|--------------|------------------------|---------------------|
| `trust`      | 0.0900                | Shuffling `trust` causes the largest drop in accuracy — it's the most critical driver in this nonlinear, test-set scenario. |
| `service`    | 0.0842                | Strong second — customers care significantly about service quality. |
| `rewarding`  | 0.0769                | Surprisingly influential, especially compared to its lower impact in linear models. Suggests **nonlinear or interaction effects**. |
| `impact`     | 0.0516                | Still meaningful, though less dominant than in Gini or regression-based results. |
| `appealing`  | 0.0475                | Contributes moderate predictive power through its aesthetic or emotional value. |
| `differs`    | 0.0298                | Differentiation is helpful, but not essential in model prediction. |
| `popular`    | 0.0240                | Weak unique signal; possibly redundant with other features. |
| `build`      | 0.0077                | Minimal contribution to predictive performance in this setting. |
| `easy`       | 0.0061                | Least influential — does not meaningfully affect satisfaction prediction when permuted. |

> Unlike Johnson weights or regression coefficients, permutation importance reflects **model behavior on real test data**, capturing **nonlinear and interaction-based influence**.
:::

```{python}
#| echo: false
results = {
    "pearson": correlations,
    "std_coef": standardized_coefs,
    "usefulness": usefulness_series,
    "johnson": johnson_weights,
    "gini": gini_importance,
    "perm": perm_importance
}
combined = pd.DataFrame({
    'Pearson Corr': results['pearson'],
    'Standardized Coef': results['std_coef'],
    'Usefulness (ΔR²)': results['usefulness'],
    "Johnson's Weight": results['johnson'],
    'Gini Importance': results['gini'],
    'Permutation Importance': results['perm']
}).round(4).sort_values(by="Johnson's Weight", ascending=False)
```

```{python}
combined
```

```{python}
#| echo: false
# Melt into long format: 4 alternatives per choice set
long_df = pd.DataFrame()

for i in range(1, 5):  # y1 to y4
    temp = pd.DataFrame({
        'id': yogurt['id'],
        'alt': i,
        'choice': yogurt[f'y{i}'],
        'feature': yogurt[f'f{i}'],
        'price': yogurt[f'p{i}']
    })
    long_df = pd.concat([long_df, temp], axis=0)

# Reset index and sort
long_df = long_df.sort_values(['id', 'alt']).reset_index(drop=True)

import statsmodels.api as sm
from statsmodels.discrete.discrete_model import MNLogit

# Create dummy variables for alt (optional: use one-hot if you want alt-specific effects)
X = long_df[['feature', 'price']]
X = sm.add_constant(X)  # add intercept
y = long_df['choice']

# Fit the MNL model using grouped choices (by id)
model = MNLogit(y, X)
result = model.fit()
print(result.summary())
```

## Discrete Choice Modeling: Multinomial Logit Model (Yogurt Data)

We estimated a **Multinomial Logit Model** (MNL) to understand what drives yogurt selection among four alternatives using `feature` and `price` as predictors.

###  Model Fit Summary
- **Log-likelihood**: –5369.8  
- **Null log-likelihood**: –5465.9  
- **Pseudo R²**: 0.0176  
- **LLR p-value**: < 0.001  
- **Convergence**: Successful after 5 iterations

While the pseudo R² is modest (1.76%), the model is **statistically significant** and helps reveal directional effects of product features on consumer choice.

### Coefficient Estimates and Interpretations

| **Variable** | **Coefficient** | **p-value** | **Interpretation** |
|--------------|------------------|-------------|---------------------|
| `const`      | –2.1173          | < 0.001     | The baseline utility (log-odds of being chosen) is low when all features are zero. |
| `feature`    | **+0.9917**      | < 0.001     | Having the feature increases the utility and thus the likelihood of this yogurt being chosen. |
| `price`      | **+11.8761**     | < 0.001     | Surprisingly, higher price **increases** the likelihood of choice. This may suggest **premium positioning** — consumers could associate price with quality. |
---
:::

```{python}
#| echo: false
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
# Create features to cluster on: either choices or price preferences
choice_behavior = yogurt[['y1', 'y2', 'y3', 'y4']]

# Normalize and cluster
scaler = StandardScaler()
X_scaled = scaler.fit_transform(choice_behavior)

kmeans = KMeans(n_clusters=2, random_state=42)
yogurt['segment'] = kmeans.fit_predict(X_scaled)

# Create long format for each segment
long_dfs = []

for i in range(1, 5):
    temp = pd.DataFrame({
        'id': yogurt['id'],
        'segment': yogurt['segment'],
        'alt': i,
        'choice': yogurt[f'y{i}'],
        'feature': yogurt[f'f{i}'],
        'price': yogurt[f'p{i}']
    })
    long_dfs.append(temp)

long_df = pd.concat(long_dfs, axis=0).sort_values(['segment', 'id', 'alt']).reset_index(drop=True)

import statsmodels.api as sm
from statsmodels.discrete.discrete_model import MNLogit

# Fit MNL model per segment
segments = long_df['segment'].unique()
results_by_segment = {}

for seg in segments:
    df_seg = long_df[long_df['segment'] == seg]
    X = sm.add_constant(df_seg[['feature', 'price']])
    y = df_seg['choice']
    
    model = MNLogit(y, X)
    result = model.fit(disp=False)
    results_by_segment[seg] = result
    print(f"Segment {seg} Results:")
    print(result.summary())
```

## Latent Segments via K-Means + Multinomial Logit Modeling

We clustered individuals based on their yogurt choice behavior and estimated **separate MNL models** per segment. Each segment represents a distinct preference pattern.

### Model Fit Overview

| Segment | Obs | Log-Likelihood | Pseudo R² | Interpretation |
|---------|-----|----------------|-----------|----------------|
| Segment 0 | 6396 | –3563.2        | 0.0093    | Weak fit, minor variation in choice explained — behavior is more uniform or driven by unmeasured factors. |
| Segment 1 | 3324 | –1375.3        | 0.2642    | Strong model fit — clearer, more structured choice patterns in this segment. |

---

### Segment-Specific Coefficients

| Segment | Intercept | Feature Coef | Price Coef | Interpretation |
|---------|-----------|---------------|-------------|----------------|
| **0**   | –0.47     | **+0.565**    | **–8.259**  | Very **price-sensitive** consumers. Slightly prefer featured products but will strongly avoid high prices. |
| **1**   | –7.37     | **+2.076**    | **+71.86**  | **Premium-seekers**: strong preference for both featured products and higher prices — possibly due to perceived quality or exclusivity. |

---

### Insights for Targeting

- **Segment 0** buyers behave like **budget-conscious deal seekers**. Appeal to them with **discounts**, **price promotions**, or **value packs**.
- **Segment 1** represents **quality-focused or brand-loyal consumers**. For this group, highlight **product benefits**, **premium packaging**, and **brand trust** — they are willing to pay more.

> These results show clear **heterogeneity in consumer preferences**, which would be missed in a single pooled MNL model. Segment-specific strategies could significantly improve targeting and conversion.
:::

```{python}
#| echo: false
# For each segment, manually pull feature and price coefficients
wtp_results = {
    "Segment 0": {
        "feature_coef": 0.5650,
        "price_coef": -8.2590
    },
    "Segment 1": {
        "feature_coef": 2.0764,
        "price_coef": 71.8603
    }
}

# Calculate WTP
for segment, coefs in wtp_results.items():
    wtp = coefs["feature_coef"] / coefs["price_coef"]
    print(f"{segment}: Willingness to Pay for 'feature' = {wtp:.4f} units of price")
```

## Willingness-to-Pay (WTP) by Segment

Willingness-to-Pay (WTP) estimates how much of a price difference consumers are willing to accept for getting the featured version of a yogurt product.

| **Segment** | **WTP (Δprice per feature)** | **Interpretation** |
|-------------|-------------------------------|---------------------|
| Segment 0   | –0.0684                       | **Price-sensitive** consumers: this group values the feature modestly but only accepts it when paired with a lower price. They effectively require a discount to choose featured options. |
| Segment 1   | +0.0289                       | **Premium-oriented** consumers: this segment is willing to pay slightly more for products with the feature — possibly due to perceived quality, prestige, or brand signaling. |

> These WTP values are calculated as:
> \[
> \text{WTP} = \frac{\beta_{\text{feature}}}{\beta_{\text{price}}}
> \]
> and expressed in **relative price units**. A positive value means the feature increases perceived value; negative implies the opposite.
:::

## Product Features that Most Influence Choice

Based on discrete choice modeling:

| **Feature** | **Effect (Pooled Model)** | **Segment 0**         | **Segment 1**         | **Interpretation** |
|-------------|---------------------------|------------------------|------------------------|---------------------|
| Price       | +11.88                    | **–8.26**              | **+71.86**             | Highly influential but varies: price is negative for value-seekers, positive for premium-seekers. |
| Feature     | +0.99                     | **+0.57**              | **+2.08**              | Consistently preferred across both segments. |
| Brand       | TBD                       | Add if available       | Add if available       | Include if you add brand-specific effects. |
| Flavor      | TBD                       | Add if available       | Add if available       | Include if present in the dataset. |

>  The most influential features are **price** and **feature**, with **segment-specific preferences** highlighting the need for **targeted product strategies**.
:::

```{python}
#| echo: false
# Drop rows with missing values
penguins = penguins.dropna()

# Select physical features only
features = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
X = penguins[features]

# Standardize
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Run K-Means
kmeans = KMeans(n_clusters=3, random_state=42)
penguins['cluster'] = kmeans.fit_predict(X_scaled)

from sklearn.mixture import GaussianMixture

gmm = GaussianMixture(n_components=3, random_state=42)
penguins['gmm_cluster'] = gmm.fit_predict(X_scaled)
```

```{python}
import seaborn as sns
# Compare clusters vs true species
sns.pairplot(penguins, hue='cluster', vars=features, palette='Set1')
plt.suptitle("K-Means Clustering of Penguins (by Physical Features)", y=1.02)
plt.show()

# Confusion matrix
from sklearn.metrics import confusion_matrix
import numpy as np

print("KMeans vs Actual Species:")
print(pd.crosstab(penguins['cluster'], penguins['species']))
```

## Unsupervised Clustering of Penguins Using K-Means

We applied **K-Means clustering** to group penguins based on physical features: `bill length`, `bill depth`, `flipper length`, and `body mass`. The algorithm formed 3 clusters, which we compared to actual species labels.

### Confusion Matrix: KMeans Cluster vs Species

| **Cluster** | **Adelie** | **Chinstrap** | **Gentoo** |
|-------------|------------|----------------|-------------|
| 0           | 124        | 5              | 0           |
| 1           | 0          | 0              | 119         |
| 2           | 22         | 63             | 0           |

---

### Interpretation

- **Cluster 0** → Dominated by **Adelie** penguins. This cluster likely reflects small-to-medium birds with deeper bills.
- **Cluster 1** → Pure **Gentoo** cluster. These penguins are larger with longer flippers and heavier bodies, which K-Means cleanly isolates.
- **Cluster 2** → Mixed cluster of **Chinstrap** and some misclassified **Adelie** penguins. This group shows overlapping traits — suggesting that **Chinstrap** features are less distinct from some Adelies.

---

### Insights

- **K-Means recovered species structure** remarkably well from physical measurements alone.
- **Gentoo penguins** are easiest to separate due to distinctive size.
- **Adelie vs Chinstrap** overlap more, which may require additional features (e.g., location, nesting patterns) for better separation.

> This analysis demonstrates how **unsupervised learning** can reveal meaningful biological segmentation — even without species labels.
:::

```{python}
#| echo: false
from sklearn.metrics import classification_report, confusion_matrix

# Features and target
features = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
X = penguins[features]
y = penguins['species']

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features (especially for SVM)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.tree import DecisionTreeClassifier

tree = DecisionTreeClassifier(random_state=42)
tree.fit(X_train, y_train)
y_pred_tree = tree.predict(X_test)
```
```{python}
print(" Decision Tree Results")
print(confusion_matrix(y_test, y_pred_tree))
print(classification_report(y_test, y_pred_tree))
```

```{python}
from sklearn.svm import SVC

svm = SVC(kernel='rbf', C=1.0, gamma='scale')
svm.fit(X_train_scaled, y_train)
y_pred_svm = svm.predict(X_test_scaled)

print(" SVM Results")
print(confusion_matrix(y_test, y_pred_svm))
print(classification_report(y_test, y_pred_svm))
```


## Classification of Penguin Species

We trained two classification models to predict penguin species from body measurements:

- `bill_length_mm`
- `bill_depth_mm`
- `flipper_length_mm`
- `body_mass_g`

### Model Accuracy

| Model           | Accuracy | Macro Precision | Macro Recall |
|-----------------|----------|------------------|---------------|
| Decision Tree   | 0.96     | 0.96             | 0.95          |
| SVM (RBF)       | 0.98     | 0.99             | 0.97          |

### Confusion Matrices

**SVM (RBF kernel)**

| Actual \ Pred | Adelie | Chinstrap | Gentoo |
|---------------|--------|-----------|--------|
| Adelie        | 48     | 0         | 0      |
| Chinstrap     | 2      | 21        | 0      |
| Gentoo        | 0      | 0         | 29     |

**Decision Tree**

| Actual \ Pred | Adelie | Chinstrap | Gentoo |
|---------------|--------|-----------|--------|
| Adelie        | 47     | 1         | 0      |
| Chinstrap     | 3      | 20        | 0      |
| Gentoo        | 0      | 0         | 29     |

### Interpretation

- Both models perform well with high accuracy.
- Gentoo penguins are classified perfectly by both.
- SVM slightly outperforms the decision tree, especially in distinguishing Chinstrap.
- Decision Trees offer interpretable rules; SVM handles overlapping boundaries better.

### Conclusion

Body size features alone are effective for species prediction. These models can support biological research, field classification, or educational tools.
:::